{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## loading required data and libs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "! pip install kaggle\n",
    "! mkdir ~/.kaggle\n",
    "! cp \"environment/kaggle.json\" ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "! kaggle competitions download -c AI4Code\n",
    "! mkdir data\n",
    "! unzip -n AI4Code.zip -d data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "! pip install -r environment/requirenments.txt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "import json\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "import pylev\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import sys\n",
    "import textwrap\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pd.options.display.width = 180\n",
    "pd.options.display.max_colwidth = 120\n",
    "\n",
    "data_dir = Path('data/')\n",
    "sys.path.append(str('AI4Code'))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6blrtiiy) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">exploration</strong>: <a href=\"https://wandb.ai/robertoo/AI4Code/runs/6blrtiiy\" target=\"_blank\">https://wandb.ai/robertoo/AI4Code/runs/6blrtiiy</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220623_200722-6blrtiiy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6blrtiiy). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/AI4code/wandb/run-20220623_200957-296eeuku</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/robertoo/AI4Code/runs/296eeuku\" target=\"_blank\">exploration</a></strong> to <a href=\"https://wandb.ai/robertoo/AI4Code\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/robertoo/AI4Code/runs/296eeuku?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fd3291659d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "# from spacy import LanguageDetector\n",
    "CONFIG = {'competition': 'AI4Code', '_wandb_kernel': 'aot'}\n",
    "wandb.init(project='AI4Code', name='exploration', config=CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## text utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2):\n",
    "    \"Find intersection between two strings\"\n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Cleaning and parsing the text.\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    nopunc = clean_text(text)\n",
    "    tokenized_text = tokenizer.tokenize(nopunc)\n",
    "    combined_text = ' '.join(tokenized_text)\n",
    "    return combined_text\n",
    "\n",
    "def clean_code(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.replace('[', ' ').replace(']', ' ').replace('(', ' ').replace(')', ' ').replace('{', ' ').replace('}', ' ').replace('=', ' ').replace(',', ' ')\n",
    "    text = text.lower()\n",
    "    text = text.replace('_', '')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('.', ' ')\n",
    "    text = re.sub(r'\".*\"', ' ', text)\n",
    "    text = re.sub(r\"'.*'\", ' ', text)\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def code_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Cleaning and parsing the text.\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    nopunc = clean_code(text)\n",
    "    tokenized_text = tokenizer.tokenize(nopunc)\n",
    "    combined_text = ' '.join(tokenized_text)\n",
    "    return combined_text\n",
    "\n",
    "def count_hastags(row):\n",
    "    \"Count the number of hashtags \"\n",
    "    row['hash_count'] = row['source'].count('# ') if row['cell_type']=='markdown' else 0\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## reading utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ranks(base, derived):\n",
    "    return [base.index(d) for d in derived]\n",
    "\n",
    "def read_train_data(data_dir, NUM_TRAIN = 10000):\n",
    "    def read_notebook(path):\n",
    "        return (\n",
    "            pd.read_json(\n",
    "                path,\n",
    "                dtype={'cell_type': 'category', 'source': 'str'})\n",
    "            .assign(id=path.stem)  # final path component\n",
    "            .rename_axis('cell_id')\n",
    "        )\n",
    "\n",
    "    paths_train = list((data_dir / 'train').glob('*.json'))[:NUM_TRAIN]\n",
    "    notebooks_train = [\n",
    "      read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')\n",
    "    ]\n",
    "    df = (\n",
    "      pd.concat(notebooks_train)\n",
    "      .set_index('id', append=True)\n",
    "      .swaplevel()\n",
    "      .sort_index(level='id', sort_remaining=False)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def get_df_orders_and_ranks(df, data_dir):\n",
    "    # train orders\n",
    "    df_orders = pd.read_csv(\n",
    "      data_dir / 'train_orders.csv',\n",
    "      index_col='id',\n",
    "      squeeze=True,\n",
    "    ).str.split()  # cell_ids str -> list\n",
    "\n",
    "\n",
    "    df_orders_ = df_orders.to_frame().join(\n",
    "      # reset only one index out of many -> \"cell_id\"; make a list out of cells in train data\n",
    "      df.reset_index('cell_id').groupby('id')['cell_id'].apply(list),\n",
    "      how='right',\n",
    "    )\n",
    "\n",
    "    ranks = {}\n",
    "    for id_, cell_order, cell_id in df_orders_.itertuples():\n",
    "        ranks[id_] = {'cell_id': cell_id, 'rank': get_ranks(cell_order, cell_id)}\n",
    "\n",
    "    df_ranks = (\n",
    "      pd.DataFrame\n",
    "      .from_dict(ranks, orient='index')\n",
    "      .rename_axis('id')\n",
    "      .apply(pd.Series.explode)\n",
    "      .set_index('cell_id', append=True)\n",
    "    )\n",
    "    # now we have\n",
    "    # id cell_id rank\n",
    "    return df_orders, df_ranks\n",
    "\n",
    "\n",
    "def get_ancestors(data_dir, ids):\n",
    "    # Split, keeping notebooks with a common origin (ancestor_id) together\n",
    "    df_ancestors = pd.read_csv(data_dir / 'train_ancestors.csv', index_col='id')\n",
    "    return df_ancestors.loc[ids, 'ancestor_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train NBs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [02:00<00:00, 82.75it/s]\n"
     ]
    }
   ],
   "source": [
    "df = read_train_data(data_dir, NUM_TRAIN=10000)\n",
    "df_orders, df_ranks = get_df_orders_and_ranks(df, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing\n",
    "https://www.kaggle.com/code/parulpandey/eda-and-preprocessing-for-bert\n",
    "\n",
    "https://www.kaggle.com/code/andradaolteanu/ai4code-language-detection-and-model-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## cleaning with regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdowns = df[df['cell_type'] == 'markdown']\n",
    "codes = df[df['cell_type'] == 'code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes.loc[:, 'source_clean'] = codes['source'].apply(str).apply(lambda x: code_preprocessing(x)).copy().values\n",
    "markdowns.loc[:, 'source_clean'] = markdowns['source'].apply(str).apply(lambda x: text_preprocessing(x)).copy().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([codes, markdowns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## counting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text_len'] = df['source_clean'].astype(str).apply(len)\n",
    "df['text_word_count'] = df['source_clean'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_XGBRanker():\n",
    "    \n",
    "    config_defaults = {\"booster\":'gbtree',\n",
    "                   \"objective\":'rank:pairwise',\n",
    "                   \"random_state\":24, \n",
    "                   \"learning_rate\":0.1,\n",
    "                   \"n_estimators\":110}\n",
    "    \n",
    "    # ðŸ W&B Experiment\n",
    "    config_defaults.update(CONFIG)\n",
    "    run = wandb.init(project='AI4Code', name='xgbRanker', config=config_defaults)\n",
    "    config = wandb.config\n",
    "    \n",
    "    # Initiate the model\n",
    "    model = XGBRanker(booster=config.booster,\n",
    "                      objective=config.objective,\n",
    "                      random_state=config.random_state, \n",
    "                      learning_rate=config.learning_rate,\n",
    "                      n_estimators=config.n_estimators)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, group=groups, verbose=True)\n",
    "\n",
    "    # Create df containing the cell_id and the prediction\n",
    "    predict = pd.DataFrame({\"cell_id\" : df_valid[\"cell_id\"],\n",
    "                            \"pred\" : model.predict(X_valid)}, index = df_valid.index)\n",
    "\n",
    "    # Sort (using the predicted rank) and then group\n",
    "    predict = predict.sort_values(by = ['id', 'pred'], ascending = [False, True])\\\n",
    "                        .groupby('id')['cell_id'].apply(list)\n",
    "\n",
    "    # Create the same but for actual data\n",
    "    actual = df_valid.sort_values(by = ['id', 'rank'], ascending = [False, True])\\\n",
    "                            .groupby('id')['cell_id'].apply(list)\n",
    "\n",
    "    # Kendall Metric\n",
    "    metric = kendall_tau(actual, predict)\n",
    "    print(\"Kendall Tau: \"+ metric)\n",
    "    wandb.log({\"kendall_tau\": np.float(metric)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_language(ID):\n",
    "    '''\n",
    "    Returns the language of the document.\n",
    "    ID: name of file\n",
    "    return :: dictionary containing the language and score (probability)\n",
    "    '''\n",
    "    # Retrieve .json df\n",
    "    df = get_json_data(ID)\n",
    "\n",
    "    # Get a string of all doc text\n",
    "    # Keep only first 200 chars to not overload memory\n",
    "    all_doc_text = \" \".join(df[df[\"cell_type\"]==\"markdown\"][\"source\"].tolist())[:200]\n",
    "\n",
    "    # Get document language\n",
    "    doc = nlp_model(all_doc_text)\n",
    "    language = doc._.language\n",
    "    \n",
    "    return language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_json_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-65c2759c421a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# This takes ~ 1hr 15 mins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mID\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_orders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mall_languages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_document_language\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Convert to dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-0fdf877d8ee0>\u001b[0m in \u001b[0;36mget_document_language\u001b[0;34m(ID)\u001b[0m\n\u001b[1;32m      6\u001b[0m     '''\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Retrieve .json df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_json_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Get a string of all doc text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_json_data' is not defined"
     ]
    }
   ],
   "source": [
    "all_languages = []\n",
    "\n",
    "# This takes ~ 1hr 15 mins\n",
    "for k, ID in tqdm(enumerate(df_orders.reset_index()[\"id\"])):\n",
    "    all_languages.append(get_document_language(ID))\n",
    "    \n",
    "# Convert to dataframe\n",
    "all_lang_df = pd.DataFrame(all_languages)\n",
    "all_lang_df[\"id\"] = df_orders.reset_index()[\"id\"]\n",
    "\n",
    "# Save file\n",
    "# .parquet is smaller than .csv\n",
    "all_lang_df.to_parquet(\"all_languages.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning glossary feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/huggingface_hub/file_download.py:563: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_glossary = pd.read_csv(\"machine_learning_glossary_terms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences we will be searching through\n",
    "corpus = np.array(ml_glossary['definition'])\n",
    "terms = np.array(ml_glossary['term'])\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiment"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Query sentences -> our sentences from the dataset to make comparison to\n",
    "queries = df[df['cell_type']==\"markdown\"]['source_clean'].values\n",
    "\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "top_k = 3\n",
    "for query in queries[:5]:\n",
    "    print(\"Query:\", query)\n",
    "    print(\"--\"*10)\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)\n",
    "    hits = hits[0]\n",
    "    print([hit['corpus_id'] for hit in hits])\n",
    "    break\n",
    "    for hit in hits:\n",
    "        print(terms[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))\n",
    "    print(\"\\n\\n======================\\n\\n\")\n",
    "    \n",
    "\"\"\"\n",
    "Query: predict fvc\n",
    "--------------------\n",
    "Predictor Variable (Score: 0.3445)\n",
    "F-Score (Score: 0.3323)\n",
    "True Positive (Score: 0.3116)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### implementing feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_vector = np.zeros_like(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k=7\n",
    "def get_top_glossary_terms(query):\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)\n",
    "    hits = [hit['corpus_id'] for hit in hits[0]]\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n",
      "+10000\n"
     ]
    }
   ],
   "source": [
    "result_vector = np.zeros(shape=(len(df), len(terms)))\n",
    "for i, text in enumerate(df['source_clean']):\n",
    "    if i and i % 10000 == 0:\n",
    "        print(\"+10000\")\n",
    "    posititions = get_top_glossary_terms(text)\n",
    "    result_vector[i, posititions] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_vector = np.load(\"glossary_matrix\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# make a use of it and save and don't calculate twice\n",
    "np.save(open(\"glossary_matrix\", 'wb'), result_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.DataFrame(result_vector).set_index(df.index)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-58bac75f4078>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pct_rank'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cell_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rank\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['pct_rank'] = df.groupby([\"id\", \"cell_type\"])[\"rank\"].apply(lambda s: pd.Series((np.arange(len(s)) + 1) /(len(s) + 1), index=s.index))\n",
    "df['pred'] = np.random.uniform(size=df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart splitting via group shuffling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "todo change to crossvalidation with groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train: 416701; validation: 44881\n"
     ]
    }
   ],
   "source": [
    "NVALID = 0.1  # size of validation set\n",
    "\n",
    "splitter = GroupShuffleSplit(n_splits=1, test_size=NVALID, random_state=0)\n",
    "\n",
    "ids = df.index.unique('id')  # get all the unique ids\n",
    "ancestors = get_ancestors(data_dir, ids)  # find ancestor by id if it exists\n",
    "# split the ids using groups. This way the same group/notebooks will be in the test or in the training\n",
    "ids_train, ids_valid = next(splitter.split(ids, groups=ancestors)) \n",
    "ids_train, ids_valid = ids[ids_train], ids[ids_valid]\n",
    "\n",
    "df_train = df.loc[ids_train, :]\n",
    "df_valid = df.loc[ids_valid, :]\n",
    "\n",
    "print(f\"Shape of train: {df_train.shape[0]}; validation: {df_valid.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_each_cell_with_its_position(current_X, full_df):\n",
    "    \"\"\"\n",
    "    marking each cell with its number if its code, for markdown zero. \n",
    "    We are doing it to help the model learn the correct order in lines?\n",
    "    \"\"\"\n",
    "    old_shape = current_X.shape\n",
    "    current_X = sparse.hstack((\n",
    "        current_X,\n",
    "        np.where(\n",
    "            full_df['cell_type'] == 'code',\n",
    "            full_df.groupby(['id', 'cell_type']).cumcount().to_numpy() + 1,\n",
    "            0,\n",
    "        ).reshape(-1, 1)\n",
    "    ))\n",
    "    new_shape = current_X.shape\n",
    "    print(f\"cell with position change {old_shape} -> {new_shape}\")\n",
    "    return current_X\n",
    "\n",
    "# idf(t) = log [ n / df(t) ] + 1, where df(t) â€“ number of time term is used\n",
    "tfidf = TfidfVectorizer(min_df=0.01, max_features=169)\n",
    "def convert_to_TfidfVector(df):\n",
    "    print(\"Converting with Tfid vectorizer\")\n",
    "    return tfidf.fit_transform(df.astype(str))\n",
    "\n",
    "def add_data_to_sparse(current_X, values):\n",
    "    print(f\"Added {values.shape} to the dataframe\")\n",
    "    return sparse.hstack((\n",
    "        current_X,\n",
    "        values\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting with Tfid vectorizer\n",
      "cell with position change (416701, 169) -> (416701, 170)\n",
      "Added (416701, 141) to the dataframe\n"
     ]
    }
   ],
   "source": [
    "# use sklearn pipeline\n",
    "X_train = convert_to_TfidfVector(df_train['source_clean'])\n",
    "X_train = mark_each_cell_with_its_position(X_train, df_train)\n",
    "X_train = add_data_to_sparse(X_train, df_train.iloc[:, 5:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_ranks.loc[ids_train].to_numpy()  # get all required train results\n",
    "groups = df_ranks.loc[ids_train].groupby('id').size().to_numpy() # Number of cells in each notebook. will later be used to help xgboost make a ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting with Tfid vectorizer\n",
      "cell with position change (44881, 169) -> (44881, 170)\n",
      "Added (44881, 141) to the dataframe\n"
     ]
    }
   ],
   "source": [
    "X_valid = convert_to_TfidfVector(df_valid['source_clean'])\n",
    "X_valid = mark_each_cell_with_its_position(X_valid, df_valid)\n",
    "X_valid = add_data_to_sparse(X_valid, df_valid.iloc[:, 5:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = df_orders.loc[ids_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame({'rank': model.predict(X_valid)}, index=df_valid.index)\n",
    "y_pred = (\n",
    "    y_pred\n",
    "    .sort_values(['id', 'rank'])  # Sort the cells in each notebook by their rank.\n",
    "                                  # The cell_ids are now in the order the model predicted.\n",
    "    .reset_index('cell_id')  # Convert the cell_id index into a column.\n",
    "    .groupby('id')['cell_id'].apply(list)  # Group the cell_ids for each notebook into a list.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## metrics utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bisect import bisect\n",
    "\n",
    "\n",
    "def count_inversions(a):\n",
    "    inversions = 0\n",
    "    sorted_so_far = []\n",
    "    for i, u in enumerate(a):\n",
    "        j = bisect(sorted_so_far, u)\n",
    "        inversions += i - j\n",
    "        sorted_so_far.insert(j, u)\n",
    "    return inversions\n",
    "\n",
    "\n",
    "def kendall_tau(ground_truth, predictions):\n",
    "    total_inversions = 0\n",
    "    total_2max = 0  # twice the maximum possible inversions across all instances\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth\n",
    "        total_inversions += count_inversions(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return 1 - 4 * total_inversions / total_2max\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4235684899561677"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dummy = df_valid.reset_index('cell_id').groupby('id')['cell_id'].apply(list)\n",
    "kendall_tau(y_valid, y_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09227517107116423"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kendall_tau(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>cell_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cell_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b47bfadb</th>\n",
       "      <td># 786\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kag...</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61f7a798</th>\n",
       "      <td># Data Loading &amp; Preparation</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958b1c80</th>\n",
       "      <td>dt = pd.read_csv(\"../input/pakistans-largest-ecommerce-dataset/Pakistan Largest Ecommerce Dataset.csv\", parse_dates=...</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f548fd2b</th>\n",
       "      <td>print(dt.info())</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b7b273f0</th>\n",
       "      <td>Data contains 1048574 rows but maximum columns contain 584524 records. \\n\\nHalf of row are completely empty, so we w...</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5e4332aa</th>\n",
       "      <td>data = dt[['sku','category_name_1']]</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61f2b3a5</th>\n",
       "      <td>data.head(20)</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cb2f35ae</th>\n",
       "      <td>data.category_name_1.value_counts()</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94601e79</th>\n",
       "      <td>As we can see above, few columns are not in correct data type. We need to perform casting.</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fa514513</th>\n",
       "      <td>data.category_name_1.['Fashion'] = data.category_name_1.['Men's Fashion','Women's Fashion']</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da00ec80</th>\n",
       "      <td>dt.tail()</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92baa943</th>\n",
       "      <td>### Let's look into summary of data\\nData Summary of non-numeric data</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0f42c05</th>\n",
       "      <td>data.describe()</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a406afa6</th>\n",
       "      <td>Data Summary of non-numeric data</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b67808dd</th>\n",
       "      <td>p.head()</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8a18dcbd</th>\n",
       "      <td>As we analysed above, we need to drop cancelled orders\\n</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           source cell_type\n",
       "cell_id                                                                                                                                    \n",
       "b47bfadb  # 786\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kag...      code\n",
       "61f7a798                                                                                             # Data Loading & Preparation  markdown\n",
       "958b1c80  dt = pd.read_csv(\"../input/pakistans-largest-ecommerce-dataset/Pakistan Largest Ecommerce Dataset.csv\", parse_dates=...      code\n",
       "f548fd2b                                                                                                         print(dt.info())      code\n",
       "b7b273f0  Data contains 1048574 rows but maximum columns contain 584524 records. \\n\\nHalf of row are completely empty, so we w...  markdown\n",
       "5e4332aa                                                                                     data = dt[['sku','category_name_1']]      code\n",
       "61f2b3a5                                                                                                            data.head(20)      code\n",
       "cb2f35ae                                                                                      data.category_name_1.value_counts()      code\n",
       "94601e79                               As we can see above, few columns are not in correct data type. We need to perform casting.  markdown\n",
       "fa514513                              data.category_name_1.['Fashion'] = data.category_name_1.['Men's Fashion','Women's Fashion']      code\n",
       "da00ec80                                                                                                                dt.tail()      code\n",
       "92baa943                                                    ### Let's look into summary of data\\nData Summary of non-numeric data  markdown\n",
       "c0f42c05                                                                                                          data.describe()      code\n",
       "a406afa6                                                                                         Data Summary of non-numeric data  markdown\n",
       "b67808dd                                                                                                                 p.head()      code\n",
       "8a18dcbd                                                                 As we analysed above, we need to drop cancelled orders\\n  markdown"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>cell_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cell_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b7b273f0</th>\n",
       "      <td>Data contains 1048574 rows but maximum columns contain 584524 records. \\n\\nHalf of row are completely empty, so we w...</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b67808dd</th>\n",
       "      <td>p.head()</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61f2b3a5</th>\n",
       "      <td>data.head(20)</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f548fd2b</th>\n",
       "      <td>print(dt.info())</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92baa943</th>\n",
       "      <td>### Let's look into summary of data\\nData Summary of non-numeric data</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94601e79</th>\n",
       "      <td>As we can see above, few columns are not in correct data type. We need to perform casting.</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8a18dcbd</th>\n",
       "      <td>As we analysed above, we need to drop cancelled orders\\n</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5e4332aa</th>\n",
       "      <td>data = dt[['sku','category_name_1']]</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fa514513</th>\n",
       "      <td>data.category_name_1.['Fashion'] = data.category_name_1.['Men's Fashion','Women's Fashion']</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da00ec80</th>\n",
       "      <td>dt.tail()</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0f42c05</th>\n",
       "      <td>data.describe()</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a406afa6</th>\n",
       "      <td>Data Summary of non-numeric data</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61f7a798</th>\n",
       "      <td># Data Loading &amp; Preparation</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958b1c80</th>\n",
       "      <td>dt = pd.read_csv(\"../input/pakistans-largest-ecommerce-dataset/Pakistan Largest Ecommerce Dataset.csv\", parse_dates=...</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cb2f35ae</th>\n",
       "      <td>data.category_name_1.value_counts()</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b47bfadb</th>\n",
       "      <td># 786\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kag...</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           source cell_type\n",
       "cell_id                                                                                                                                    \n",
       "b7b273f0  Data contains 1048574 rows but maximum columns contain 584524 records. \\n\\nHalf of row are completely empty, so we w...  markdown\n",
       "b67808dd                                                                                                                 p.head()      code\n",
       "61f2b3a5                                                                                                            data.head(20)      code\n",
       "f548fd2b                                                                                                         print(dt.info())      code\n",
       "92baa943                                                    ### Let's look into summary of data\\nData Summary of non-numeric data  markdown\n",
       "94601e79                               As we can see above, few columns are not in correct data type. We need to perform casting.  markdown\n",
       "8a18dcbd                                                                 As we analysed above, we need to drop cancelled orders\\n  markdown\n",
       "5e4332aa                                                                                     data = dt[['sku','category_name_1']]      code\n",
       "fa514513                              data.category_name_1.['Fashion'] = data.category_name_1.['Men's Fashion','Women's Fashion']      code\n",
       "da00ec80                                                                                                                dt.tail()      code\n",
       "c0f42c05                                                                                                          data.describe()      code\n",
       "a406afa6                                                                                         Data Summary of non-numeric data  markdown\n",
       "61f7a798                                                                                             # Data Loading & Preparation  markdown\n",
       "958b1c80  dt = pd.read_csv(\"../input/pakistans-largest-ecommerce-dataset/Pakistan Largest Ecommerce Dataset.csv\", parse_dates=...      code\n",
       "cb2f35ae                                                                                      data.category_name_1.value_counts()      code\n",
       "b47bfadb  # 786\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kag...      code"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_id = df_valid.index.get_level_values('id').unique()[8]\n",
    "\n",
    "display(df.loc[nb_id].loc[y_valid.loc[nb_id]][['source', 'cell_type']])\n",
    "display(df.loc[nb_id].loc[y_pred.loc[nb_id]][['source', 'cell_type']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
