{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b08f61-dc90-46ca-a234-fc31336ec004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -U -r environment/requirenments.txt\n",
    "! pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "! python -m spacy download en_core_web_lg\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf4a4002-5628-483e-99fd-3004b4d7e526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "# from autocorrect import Speller\n",
    "import json\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import tokenize\n",
    "from tokenize import TokenError\n",
    "import io\n",
    "tqdm.pandas()\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import sys\n",
    "import textwrap\n",
    "import wandb\n",
    "# import spacy\n",
    "# import contextualSpellCheck\n",
    "from bisect import bisect\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# ner = spacy.load(\"en_core_web_lg\")\n",
    "# check_spelling = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pd.options.display.width = 180\n",
    "pd.options.display.max_colwidth = 120\n",
    "\n",
    "data_dir = Path('data/')\n",
    "sys.path.append(str('AI4Code'))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d00cec94-6898-4f15-a752-e8ef05fdc275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranks(base, derived):\n",
    "    return [base.index(d) for d in derived]\n",
    "\n",
    "def read_train_data(data_dir, NUM_TRAIN = 10000, OFFSET=0):\n",
    "    def read_notebook(path):\n",
    "        return (\n",
    "            pd.read_json(\n",
    "                path,\n",
    "                dtype={'cell_type': 'category', 'source': 'str'})\n",
    "            .assign(id=path.stem)  # final path component\n",
    "            .rename_axis('cell_id')\n",
    "        )\n",
    "\n",
    "    paths_train = list((data_dir / 'train').glob('*.json'))[OFFSET:NUM_TRAIN]\n",
    "    notebooks_train = [\n",
    "      read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')\n",
    "    ]\n",
    "    df = (\n",
    "      pd.concat(notebooks_train)\n",
    "      .set_index('id', append=True)\n",
    "      .swaplevel()\n",
    "      .sort_index(level='id', sort_remaining=False)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def get_df_orders_and_ranks(df, data_dir):\n",
    "    # train orders\n",
    "    df_orders = pd.read_csv(\n",
    "      data_dir / 'train_orders.csv',\n",
    "      index_col='id',\n",
    "      squeeze=True,\n",
    "    ).str.split()  # cell_ids str -> list\n",
    "\n",
    "\n",
    "    df_orders_ = df_orders.to_frame().join(\n",
    "      # reset only one index out of many -> \"cell_id\"; make a list out of cells in train data\n",
    "      df.reset_index('cell_id').groupby('id')['cell_id'].apply(list),\n",
    "      how='right',\n",
    "    )\n",
    "\n",
    "    ranks = {}\n",
    "    for id_, cell_order, cell_id in df_orders_.itertuples():\n",
    "        ranks[id_] = {'cell_id': cell_id, 'rank': get_ranks(cell_order, cell_id)}\n",
    "\n",
    "    df_ranks = (\n",
    "      pd.DataFrame\n",
    "      .from_dict(ranks, orient='index')\n",
    "      .rename_axis('id')\n",
    "      .apply(pd.Series.explode)\n",
    "      .set_index('cell_id', append=True)\n",
    "    )\n",
    "    # now we have\n",
    "    # id cell_id rank\n",
    "    return df_orders, df_ranks\n",
    "\n",
    "\n",
    "def get_ancestors(data_dir, ids):\n",
    "    # Split, keeping notebooks with a common origin (ancestor_id) together\n",
    "    df_ancestors = pd.read_csv(data_dir / 'train_ancestors.csv', index_col='id')\n",
    "    return df_ancestors.loc[ids, 'ancestor_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6b080-36ef-46a1-adb2-483c9cf33761",
   "metadata": {},
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df7f93-6b88-4f0c-bf6f-08581991bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the parameters\n",
    "NVALID = 0.1  # size of validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a65892cf-420a-4d7f-8054-5e618eae18b7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f682beee7b4b5e82ad5c9de9b0585f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train NBs', max=5000.0, style=ProgressStyle(description_wâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-3bc3cd2fca07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# example pipeline work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_TRAIN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_orders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_ranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_df_orders_and_ranks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-4b123ca0364b>\u001b[0m in \u001b[0;36mread_train_data\u001b[0;34m(data_dir, NUM_TRAIN, OFFSET)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpaths_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOFFSET\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mNUM_TRAIN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     notebooks_train = [\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mread_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train NBs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     ]\n\u001b[1;32m     18\u001b[0m     df = (\n",
      "\u001b[0;32m<ipython-input-111-4b123ca0364b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpaths_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOFFSET\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mNUM_TRAIN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     notebooks_train = [\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mread_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train NBs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     ]\n\u001b[1;32m     18\u001b[0m     df = (\n",
      "\u001b[0;32m<ipython-input-111-4b123ca0364b>\u001b[0m in \u001b[0;36mread_notebook\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      7\u001b[0m             pd.read_json(\n\u001b[1;32m      8\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 dtype={'cell_type': 'category', 'source': 'str'})\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# final path component\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mrename_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cell_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_axes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_convert_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_convert_axes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AXIS_NUMBERS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             new_axis, result = self._try_convert_data(\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_dtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m             )\n\u001b[1;32m    866\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_try_convert_data\u001b[0;34m(self, name, data, use_dtypes, convert_dates)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert_dates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m             \u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_convert_to_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_try_convert_to_date\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdate_unit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdate_units\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m                 \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_unit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, box, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_assemble_from_unit_mappings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m         \u001b[0mcache_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcache_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_and_box_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_maybe_cache\u001b[0;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0mcache_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Perform a quicker unique check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m                 raise NotImplementedError(\n\u001b[1;32m    233\u001b[0m                     \u001b[0;34m\"initializing a Series from a \"\u001b[0m \u001b[0;34m\"MultiIndex is not supported\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# example pipeline work\n",
    "df = read_train_data(data_dir, NUM_TRAIN=1000)\n",
    "df_orders, df_ranks = get_df_orders_and_ranks(df, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6983eec-0e82-4e17-82e4-c000e4fb9846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting on markdown and code\n",
    "markdowns = df[df['cell_type'] == 'markdown']\n",
    "codes = df[df['cell_type'] == 'code']\n",
    "df = pd.concat([codes, markdowns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c483559-fabd-4f12-84db-b2066dde39c2",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eb40be-d66a-4ccd-a6fd-bb5f7586d7e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=1, test_size=NVALID, random_state=0)\n",
    "ids = df.index.unique('id')\n",
    "ancestors = get_ancestors(data_dir, ids)  # find ancestor by id if it exists\n",
    "ids_train, ids_valid = next(splitter.split(ids, groups=ancestors)) \n",
    "ids_train, ids_valid = ids[ids_train], ids[ids_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d35497a-f832-40b3-bc05-2b2d75de36fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = sparse.load_npz('data/df_tfisd_15k.npz').A[:len(df)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4561cece-4eaf-42d7-a005-6058fa2d5199",
   "metadata": {},
   "source": [
    "# for markdown only model\n",
    "df_train_markdown = sparse_matrix[\n",
    "    (df.index.get_level_values('id').isin(ids_train))\n",
    "    & \n",
    "    (df[\"cell_type\"] == \"markdown\")\n",
    "]\n",
    "df_valid_markdown = sparse_matrix[\n",
    "    (df.index.get_level_values('id').isin(ids_valid))\n",
    "    & \n",
    "    (df[\"cell_type\"] == \"markdown\")\n",
    "]\n",
    "print(f\"Shape of train: {df_train_markdown.shape[0]}; validation: {df_valid_markdown.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b2156f81-0988-4561-99e0-db2d764ac229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for getting original df\n",
    "df_train = df[df.index.get_level_values('id').isin(ids_train)]\n",
    "df_valid = df[df.index.get_level_values('id').isin(ids_valid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b5f887f-db7c-4022-b262-3724119b79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sparse_matrix[\n",
    "    (df.index.get_level_values('id').isin(ids_train))\n",
    "]\n",
    "X_valid = sparse_matrix[\n",
    "    (df.index.get_level_values('id').isin(ids_valid))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "336145ed-8837-4f37-9d58-8081216c4c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack((\n",
    "    X_train,\n",
    "    np.where(\n",
    "        df_train['cell_type'] == 'code',\n",
    "        df_train.groupby(['id', 'cell_type']).cumcount().to_numpy() + 1,\n",
    "        0,\n",
    "    ).reshape(-1, 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4b9278d2-bf8a-4783-a6d6-bd1777833e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = np.hstack((\n",
    "    X_valid,\n",
    "    np.where(\n",
    "        df_valid['cell_type'] == 'code',\n",
    "        df_valid.groupby(['id', 'cell_type']).cumcount().to_numpy() + 1,\n",
    "        0,\n",
    "    ).reshape(-1, 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c66188c-5c60-49e4-a3d0-d03cba592d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9248ec7c-cfc4-4964-9472-9d60307667a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all required train results\n",
    "y_train = df_ranks.loc[ids_train].to_numpy()\n",
    "# Number of cells in each notebook. will later be used to help xgboost make a ranking\n",
    "groups = df_ranks.loc[ids_train].groupby('id').size().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "195506e5-943a-4d0d-bf8f-3239cf08f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = df_orders.loc[ids_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf2e040-d102-49c5-aeee-781cad8f79eb",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e50f2-e4ee-4ae1-9bbd-44757ddb245e",
   "metadata": {},
   "source": [
    "# xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1fcc62ea-a334-4aa1-bd6a-63f51a725d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRanker\n",
    "\n",
    "model = XGBRanker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cf2b016e-59f9-41db-b985-7498a9e15a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRanker(base_score=0.5, booster='gbtree', callbacks=None, colsample_bylevel=1,\n",
       "          colsample_bynode=1, colsample_bytree=1, early_stopping_rounds=None,\n",
       "          enable_categorical=False, eval_metric=None, gamma=0, gpu_id=-1,\n",
       "          grow_policy='depthwise', importance_type=None,\n",
       "          interaction_constraints='', learning_rate=0.300000012, max_bin=256,\n",
       "          max_cat_to_onehot=4, max_delta_step=0, max_depth=6, max_leaves=0,\n",
       "          min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "          n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "          objective='rank:pairwise', predictor='auto', random_state=0,\n",
       "          reg_alpha=0, ...)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, group=groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab531c6-0800-4ac7-bf7d-84bf2a79b223",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pytorch use language code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26043884-09a5-42be-a94a-3ba5f84003df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed131a57-77c1-4b95-b2e7-5abe590ac651",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3970cd3-06f2-45ee-bd72-056c798f46e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkdownModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MarkdownModel, self).__init__()\n",
    "        self.distill_bert = DistilBertModel.from_pretrained(BERT_PATH)\n",
    "\n",
    "        self.top1 = nn.Linear(768, 64)\n",
    "        self.top2 = nn.Linear(64, 1)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(p=0.2)\n",
    "        self.dropout2 = torch.nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        x = self.distill_bert(ids, mask)[0][:, 0, :]\n",
    "        x = self.dropout1(x)\n",
    "        x0 = self.top1(x)\n",
    "        x = self.dropout2(x0)\n",
    "        x = self.top2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23a0496f-e1dc-4397-b88e-f94ee01a98f4",
   "metadata": {},
   "source": [
    "!git clone https://huggingface.co/distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a52eb60-bd84-46c3-b148-c5a5848992e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PATH = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "b96dc08e-1d27-46ec-b1f8-1db16a467685",
   "metadata": {},
   "source": [
    "class MarkdownDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, max_len):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.df[index]\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            row.source,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = torch.LongTensor(inputs['input_ids'])\n",
    "        mask = torch.LongTensor(inputs['attention_mask'])\n",
    "\n",
    "        return ids, mask, torch.FloatTensor([row.pct_rank])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "train_ds = MarkdownDataset(df_train_markdown, max_len=MAX_LEN)\n",
    "val_ds = MarkdownDataset(df_valid_markdown, max_len=MAX_LEN)\n",
    "\n",
    "val_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d5885-83ff-47e1-80ef-8665d91096ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_mark.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8bf340-d96d-4334-9d82-709589faf445",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ba75edb1-2761-4e53-b1bd-24c1291fb218",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame({'rank': model.predict(X_valid)}, index=df_valid.index)\n",
    "y_pred = (\n",
    "    y_pred\n",
    "    .sort_values(['id', 'rank'])  # Sort the cells in each notebook by their rank.\n",
    "                                  # The cell_ids are now in the order the model predicted.\n",
    "    .reset_index('cell_id')  # Convert the cell_id index into a column.\n",
    "    .groupby('id')['cell_id'].apply(list)  # Group the cell_ids for each notebook into a list.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db67efe3-d7a8-486b-bd80-d866527ee531",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "32a379e4-30a8-4beb-8732-cc5ad15a5d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_inversions(a):\n",
    "    inversions = 0\n",
    "    sorted_so_far = []\n",
    "    for i, u in enumerate(a):\n",
    "        j = bisect(sorted_so_far, u)\n",
    "        inversions += i - j\n",
    "        sorted_so_far.insert(j, u)\n",
    "    return inversions\n",
    "\n",
    "\n",
    "def kendall_tau(ground_truth, predictions):\n",
    "    total_inversions = 0\n",
    "    total_2max = 0  # twice the maximum possible inversions across all instances\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth\n",
    "        total_inversions += count_inversions(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return 1 - 4 * total_inversions / total_2max\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86a6a5-9b0d-4ee5-baad-e839ab092512",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8c041f9b-d2f4-4227-95ab-5f2b1c362d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.283291481553023"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dummy = df_valid.reset_index('cell_id').groupby('id')['cell_id'].apply(list)\n",
    "kendall_tau(y_valid, y_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "db6eee98-2b43-4ce3-9599-167594777542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.03883330113965622"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kendall_tau(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a101d-0a94-4ad5-b663-764d96ae8deb",
   "metadata": {},
   "source": [
    "## Looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "671a1007-ee26-43a5-bf61-629c02314c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>cell_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cell_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ae6e8718</th>\n",
       "      <td>You've built a model. But how good is it?\\n\\nIn this lesson, you will learn to use model validation to measure the q...</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0f981241</th>\n",
       "      <td># Data Loading Code Hidden Here\\nimport pandas as pd\\n\\n# Load data\\nmelbourne_file_path = '../input/melbourne-housi...</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>063d7406</th>\n",
       "      <td>Once we have a model, here is how we calculate the mean absolute error:</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b348f80e</th>\n",
       "      <td>from sklearn.metrics import mean_absolute_error\\n\\npredicted_home_prices = melbourne_model.predict(X)\\nmean_absolute...</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a39c4598</th>\n",
       "      <td># The Problem with \"In-Sample\" Scores\\n\\nThe measure we just computed can be called an \"in-sample\" score. We used a ...</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bd5bab25</th>\n",
       "      <td>from sklearn.model_selection import train_test_split\\n\\n# split data into training and validation data, for both fea...</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42384b5d</th>\n",
       "      <td># Wow!\\n\\nYour mean absolute error for the in-sample data was about 500 dollars.  Out-of-sample it is more than 250,...</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465e607e</th>\n",
       "      <td># Your Turn\\nBefore we look at improving this model, try **[Model Validation](https://www.kaggle.com/kernels/fork/22...</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           source cell_type\n",
       "cell_id                                                                                                                                    \n",
       "ae6e8718  You've built a model. But how good is it?\\n\\nIn this lesson, you will learn to use model validation to measure the q...  markdown\n",
       "0f981241  # Data Loading Code Hidden Here\\nimport pandas as pd\\n\\n# Load data\\nmelbourne_file_path = '../input/melbourne-housi...      code\n",
       "063d7406                                                  Once we have a model, here is how we calculate the mean absolute error:  markdown\n",
       "b348f80e  from sklearn.metrics import mean_absolute_error\\n\\npredicted_home_prices = melbourne_model.predict(X)\\nmean_absolute...      code\n",
       "a39c4598  # The Problem with \"In-Sample\" Scores\\n\\nThe measure we just computed can be called an \"in-sample\" score. We used a ...  markdown\n",
       "bd5bab25  from sklearn.model_selection import train_test_split\\n\\n# split data into training and validation data, for both fea...      code\n",
       "42384b5d  # Wow!\\n\\nYour mean absolute error for the in-sample data was about 500 dollars.  Out-of-sample it is more than 250,...  markdown\n",
       "465e607e  # Your Turn\\nBefore we look at improving this model, try **[Model Validation](https://www.kaggle.com/kernels/fork/22...  markdown"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_id = df_valid.index.get_level_values('id').unique()[13]\n",
    "\n",
    "df.loc[nb_id].loc[y_valid.loc[nb_id]][['source', 'cell_type']].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e1eac0f8-dc58-40b8-a64d-040f1bd90c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>cell_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cell_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>063d7406</th>\n",
       "      <td>Once we have a model, here is how we calculate the mean absolute error:</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465e607e</th>\n",
       "      <td># Your Turn\\nBefore we look at improving this model, try **[Model Validation](https://www.kaggle.com/kernels/fork/22...</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ae6e8718</th>\n",
       "      <td>You've built a model. But how good is it?\\n\\nIn this lesson, you will learn to use model validation to measure the q...</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a39c4598</th>\n",
       "      <td># The Problem with \"In-Sample\" Scores\\n\\nThe measure we just computed can be called an \"in-sample\" score. We used a ...</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0f981241</th>\n",
       "      <td># Data Loading Code Hidden Here\\nimport pandas as pd\\n\\n# Load data\\nmelbourne_file_path = '../input/melbourne-housi...</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bd5bab25</th>\n",
       "      <td>from sklearn.model_selection import train_test_split\\n\\n# split data into training and validation data, for both fea...</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42384b5d</th>\n",
       "      <td># Wow!\\n\\nYour mean absolute error for the in-sample data was about 500 dollars.  Out-of-sample it is more than 250,...</td>\n",
       "      <td>markdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b348f80e</th>\n",
       "      <td>from sklearn.metrics import mean_absolute_error\\n\\npredicted_home_prices = melbourne_model.predict(X)\\nmean_absolute...</td>\n",
       "      <td>code</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           source cell_type\n",
       "cell_id                                                                                                                                    \n",
       "063d7406                                                  Once we have a model, here is how we calculate the mean absolute error:  markdown\n",
       "465e607e  # Your Turn\\nBefore we look at improving this model, try **[Model Validation](https://www.kaggle.com/kernels/fork/22...  markdown\n",
       "ae6e8718  You've built a model. But how good is it?\\n\\nIn this lesson, you will learn to use model validation to measure the q...  markdown\n",
       "a39c4598  # The Problem with \"In-Sample\" Scores\\n\\nThe measure we just computed can be called an \"in-sample\" score. We used a ...  markdown\n",
       "0f981241  # Data Loading Code Hidden Here\\nimport pandas as pd\\n\\n# Load data\\nmelbourne_file_path = '../input/melbourne-housi...      code\n",
       "bd5bab25  from sklearn.model_selection import train_test_split\\n\\n# split data into training and validation data, for both fea...      code\n",
       "42384b5d  # Wow!\\n\\nYour mean absolute error for the in-sample data was about 500 dollars.  Out-of-sample it is more than 250,...  markdown\n",
       "b348f80e  from sklearn.metrics import mean_absolute_error\\n\\npredicted_home_prices = melbourne_model.predict(X)\\nmean_absolute...      code"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[nb_id].loc[y_pred.loc[nb_id]][['source', 'cell_type']].head(15)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d4fd267-30fb-484d-aa77-d129ec46ecf5",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
