{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d558bc-aa47-4518-bb63-189eadd28dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6b080-36ef-46a1-adb2-483c9cf33761",
   "metadata": {},
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df7f93-6b88-4f0c-bf6f-08581991bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the parameters\n",
    "NVALID = 0.1  # size of validation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c483559-fabd-4f12-84db-b2066dde39c2",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd60cbe-209c-4393-af93-7516ef39845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=1, test_size=NVALID, random_state=0)\n",
    "\n",
    "ids = df.index.unique('id')  # get all the unique ids\n",
    "ancestors = get_ancestors(data_dir, ids)  # find ancestor by id if it exists\n",
    "# split the ids using groups. This way the same group/notebooks will be in the test or in the training\n",
    "ids_train, ids_valid = next(splitter.split(ids, groups=ancestors)) \n",
    "ids_train, ids_valid = ids[ids_train], ids[ids_valid]\n",
    "\n",
    "df_train = df.loc[ids_train, :]\n",
    "df_valid = df.loc[ids_valid, :]\n",
    "\n",
    "print(f\"Shape of train: {df_train.shape[0]}; validation: {df_valid.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248ec7c-cfc4-4964-9472-9d60307667a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_ranks.loc[ids_train].to_numpy()  # get all required train results\n",
    "groups = df_ranks.loc[ids_train].groupby('id').size().to_numpy() # Number of cells in each notebook. will later be used to help xgboost make a ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195506e5-943a-4d0d-bf8f-3239cf08f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = df_orders.loc[ids_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba75edb1-2761-4e53-b1bd-24c1291fb218",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame({'rank': model.predict(X_valid)}, index=df_valid.index)\n",
    "y_pred = (\n",
    "    y_pred\n",
    "    .sort_values(['id', 'rank'])  # Sort the cells in each notebook by their rank.\n",
    "                                  # The cell_ids are now in the order the model predicted.\n",
    "    .reset_index('cell_id')  # Convert the cell_id index into a column.\n",
    "    .groupby('id')['cell_id'].apply(list)  # Group the cell_ids for each notebook into a list.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db67efe3-d7a8-486b-bd80-d866527ee531",
   "metadata": {},
   "source": [
    "# Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32a379e4-30a8-4beb-8732-cc5ad15a5d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_inversions(a):\n",
    "    inversions = 0\n",
    "    sorted_so_far = []\n",
    "    for i, u in enumerate(a):\n",
    "        j = bisect(sorted_so_far, u)\n",
    "        inversions += i - j\n",
    "        sorted_so_far.insert(j, u)\n",
    "    return inversions\n",
    "\n",
    "\n",
    "def kendall_tau(ground_truth, predictions):\n",
    "    total_inversions = 0\n",
    "    total_2max = 0  # twice the maximum possible inversions across all instances\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth\n",
    "        total_inversions += count_inversions(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return 1 - 4 * total_inversions / total_2max\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86a6a5-9b0d-4ee5-baad-e839ab092512",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c041f9b-d2f4-4227-95ab-5f2b1c362d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dummy = df_valid.reset_index('cell_id').groupby('id')['cell_id'].apply(list)\n",
    "kendall_tau(y_valid, y_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6eee98-2b43-4ce3-9599-167594777542",
   "metadata": {},
   "outputs": [],
   "source": [
    "kendall_tau(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a101d-0a94-4ad5-b663-764d96ae8deb",
   "metadata": {},
   "source": [
    "## Looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a1007-ee26-43a5-bf61-629c02314c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_id = df_valid.index.get_level_values('id').unique()[8]\n",
    "\n",
    "display(df.loc[nb_id].loc[y_valid.loc[nb_id]][['source', 'cell_type']])\n",
    "display(df.loc[nb_id].loc[y_pred.loc[nb_id]][['source', 'cell_type']])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d4fd267-30fb-484d-aa77-d129ec46ecf5",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
