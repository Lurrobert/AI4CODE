{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## loading required data and libs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "! pip install kaggle\n",
    "! mkdir ~/.kaggle\n",
    "! cp \"environment/kaggle.json\" ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "! kaggle competitions download -c AI4Code\n",
    "! mkdir data\n",
    "! unzip -n AI4Code.zip -d data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! pip install botocore --upgrade\n",
    "! pip install -U spacy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ! pip install -r environment/requirenments.txt\n",
    "# ! pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "! python -m spacy download en_core_web_lg\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "import json\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "# import pylev\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import tokenize\n",
    "from tokenize import TokenError\n",
    "import io\n",
    "tqdm.pandas()\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import sys\n",
    "import textwrap\n",
    "import wandb\n",
    "import spacy\n",
    "ner = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pd.options.display.width = 180\n",
    "pd.options.display.max_colwidth = 120\n",
    "\n",
    "data_dir = Path('data/')\n",
    "sys.path.append(str('AI4Code'))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG = {'competition': 'AI4Code', '_wandb_kernel': 'aot'}\n",
    "# wandb.init(project='AI4Code', name='exploration', config=CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## text utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Cleaning and parsing the text.\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    nopunc = clean_text(text)\n",
    "    tokenized_text = tokenizer.tokenize(nopunc)\n",
    "    combined_text = ' '.join(tokenized_text)\n",
    "    return combined_text\n",
    "\n",
    "def clean_code(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.replace('[', ' ').replace(']', ' ').replace('(', ' ')\\\n",
    "    .replace(')', ' ').replace('{', ' ').replace('}', ' ').replace('=', ' ').replace(',', ' ')\n",
    "    text = text.lower()\n",
    "    text = text.replace('_', ' ')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('.', ' ')\n",
    "    text = re.sub(r'\".*\"', ' ', text)\n",
    "    text = re.sub(r\"'.*'\", ' ', text)\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def code_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Cleaning and parsing the text.\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    nopunc = clean_code(text)\n",
    "    tokenized_text = tokenizer.tokenize(nopunc)\n",
    "    combined_text = ' '.join(tokenized_text)\n",
    "    return combined_text\n",
    "\n",
    "def count_hastags(row):\n",
    "    \"Count the number of hashtags \"\n",
    "    row['hash_count'] = row['source'].count('# ') if row['cell_type']=='markdown' else 0\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## reading utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ranks(base, derived):\n",
    "    return [base.index(d) for d in derived]\n",
    "\n",
    "def read_train_data(data_dir, NUM_TRAIN = 10000, OFFSET=0):\n",
    "    def read_notebook(path):\n",
    "        return (\n",
    "            pd.read_json(\n",
    "                path,\n",
    "                dtype={'cell_type': 'category', 'source': 'str'})\n",
    "            .assign(id=path.stem)  # final path component\n",
    "            .rename_axis('cell_id')\n",
    "        )\n",
    "\n",
    "    paths_train = list((data_dir / 'train').glob('*.json'))[OFFSET:NUM_TRAIN]\n",
    "    notebooks_train = [\n",
    "      read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')\n",
    "    ]\n",
    "    df = (\n",
    "      pd.concat(notebooks_train)\n",
    "      .set_index('id', append=True)\n",
    "      .swaplevel()\n",
    "      .sort_index(level='id', sort_remaining=False)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def get_df_orders_and_ranks(df, data_dir):\n",
    "    # train orders\n",
    "    df_orders = pd.read_csv(\n",
    "      data_dir / 'train_orders.csv',\n",
    "      index_col='id',\n",
    "      squeeze=True,\n",
    "    ).str.split()  # cell_ids str -> list\n",
    "\n",
    "\n",
    "    df_orders_ = df_orders.to_frame().join(\n",
    "      # reset only one index out of many -> \"cell_id\"; make a list out of cells in train data\n",
    "      df.reset_index('cell_id').groupby('id')['cell_id'].apply(list),\n",
    "      how='right',\n",
    "    )\n",
    "\n",
    "    ranks = {}\n",
    "    for id_, cell_order, cell_id in df_orders_.itertuples():\n",
    "        ranks[id_] = {'cell_id': cell_id, 'rank': get_ranks(cell_order, cell_id)}\n",
    "\n",
    "    df_ranks = (\n",
    "      pd.DataFrame\n",
    "      .from_dict(ranks, orient='index')\n",
    "      .rename_axis('id')\n",
    "      .apply(pd.Series.explode)\n",
    "      .set_index('cell_id', append=True)\n",
    "    )\n",
    "    # now we have\n",
    "    # id cell_id rank\n",
    "    return df_orders, df_ranks\n",
    "\n",
    "\n",
    "def get_ancestors(data_dir, ids):\n",
    "    # Split, keeping notebooks with a common origin (ancestor_id) together\n",
    "    df_ancestors = pd.read_csv(data_dir / 'train_ancestors.csv', index_col='id')\n",
    "    return df_ancestors.loc[ids, 'ancestor_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061b5306f16a47839f6d2117463eb69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train NBs', max=1000.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# example pipeline work\n",
    "df = read_train_data(data_dir, NUM_TRAIN=1000)\n",
    "df_orders, df_ranks = get_df_orders_and_ranks(df, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, func, new_column, on_column='source'):\n",
    "    df[new_column] = df.progress_apply(lambda x: func(x[on_column]), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#splitting on markdown and code\n",
    "markdowns = df[df['cell_type'] == 'markdown']\n",
    "codes = df[df['cell_type'] == 'code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## lemmatizer \n",
    "lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def tokenize_python_code_names(text):\n",
    "    try:\n",
    "        code_text = tokenize.generate_tokens(io.StringIO(text).readline)\n",
    "        strings = [tok.string for tok in code_text if tok.type==1]\n",
    "    except Exception:\n",
    "        return None # Error happened\n",
    "    return ' '.join(strings)\n",
    "\n",
    "def tokenize_python_code_comments(text):\n",
    "    try:\n",
    "        code_text = tokenize.generate_tokens(io.StringIO(text).readline)\n",
    "        strings = [tok.string for tok in code_text if tok.type==55]\n",
    "    except Exception:\n",
    "        return None # Error happened\n",
    "    return ' '.join(strings)\n",
    "\n",
    "def lemm_sentence(text):\n",
    "    lst_txt = [lem.lemmatize(word) for word in text.split()]\n",
    "    return ' '.join(lst_txt)\n",
    "\n",
    "def collect_entities(text):\n",
    "    \"Named Entity Recognition\"\n",
    "    rs = ner(text)\n",
    "    labels = []\n",
    "    for r in rs.ents:\n",
    "        labels.append(r.label_)\n",
    "    return ' '.join(labels)\n",
    "# Some example of results on 1000 ds\n",
    "# [['CARDINAL' '851']\n",
    "#  ['DATE' '561']\n",
    "#  ['EVENT' '6']\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3be2e52c3274b0bbbbb730088347832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30897.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Pipeline]  (step 1 of 6) Processing Clearing and parsing source, total=   4.6s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c9fbafb7074726b5549f03031ff448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30897.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Pipeline]  (step 2 of 6) Processing Tokenizing python code names, total=  11.7s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae1ac1c33cd431996b32da613886805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30897.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Pipeline]  (step 3 of 6) Processing [1] Tokenizing python code comments, total=  11.6s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2af63c519148a7b6f39c31033ef926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30897.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Pipeline]  (step 4 of 6) Processing [2] Lemmatizing code comments, total=   3.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5cb576efc9e44d7bf8d6a5d49e3bcfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30897.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Pipeline]  (step 5 of 6) Processing [3] Clearing and parsing code comments, total=   3.8s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081e0a614b6f4a9e91c48daa07756998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30897.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Pipeline]  (step 6 of 6) Processing [4] Lemmatizing comments, total=   5.6s\n"
     ]
    }
   ],
   "source": [
    "code_pipeline = Pipeline([\n",
    "    ('Clearing and parsing source', FunctionTransformer(\n",
    "         func=prepare_data,\n",
    "         kw_args={\n",
    "             'func': code_preprocessing,\n",
    "             'new_column': \"source_clean\",  # TODO maybe even remove it or leave the same. check on gridsearch later\n",
    "             'on_column': \"source\"})),\n",
    "    \n",
    "    ('Tokenizing python code names', FunctionTransformer(\n",
    "         func=prepare_data,\n",
    "         kw_args={\n",
    "             'func': tokenize_python_code_names,\n",
    "             'new_column': \"python_code_names\",\n",
    "             'on_column': \"source\"})),\n",
    "    \n",
    "    ('[1] Tokenizing python code comments', FunctionTransformer(\n",
    "         func=prepare_data,\n",
    "         kw_args={\n",
    "             'func': tokenize_python_code_comments,\n",
    "             'new_column': \"code_comments\",\n",
    "             'on_column': \"source\"})),\n",
    "    ('[2] Lemmatizing code comments', FunctionTransformer(\n",
    "         func=prepare_data,\n",
    "         kw_args={\n",
    "             'func': tokenize_python_code_comments,\n",
    "             'new_column': \"code_comments\",\n",
    "             'on_column': \"code_comments\"})),\n",
    "    ('[3] Clearing and parsing code comments', FunctionTransformer(\n",
    "         func=prepare_data,\n",
    "         kw_args={\n",
    "             'func': text_preprocessing, # todo maybe make additional pipeline for comments and multisteps pipe\n",
    "             'new_column': \"code_comments\",\n",
    "             'on_column': \"code_comments\"})),\n",
    "    ('[4] Lemmatizing comments', FunctionTransformer(\n",
    "         func=prepare_data,\n",
    "         kw_args={\n",
    "             'func': lemm_sentence,\n",
    "             'new_column': \"code_comments_lemm\",\n",
    "             'on_column': \"code_comments\"})),\n",
    "    # todo make comments statistics and implement it if column is not empty\n",
    "    # ('[3] Comments stats', FunctionTransformer(\n",
    "    #      func=prepare_data,\n",
    "    #      kw_args={\n",
    "    #          'func': lambda x: len(str(x).split()),\n",
    "    #          'new_column': \"len_of_code_comments\",\n",
    "    #          'on_column': \"code_comments\"})),\n",
    "], verbose=True)\n",
    "result = code_pipeline.fit_transform(codes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install contextualSpellCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextualSpellCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cebea14ee474890a2ce803f6412c1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b283841ba2df45d88a0af076f00f7e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a92714629a41658f3074013c222f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6ea445ebc24550b0cb2bf25194c4d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6d14a56dda487fb614b4edf321c63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "660 ms ± 7.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "contextualSpellCheck.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://github.com/filyp/autocorrect\n",
    "https://spacy.io/universe/project/contextualSpellCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691 ms ± 8.17 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit nlp('There is no comin to consiousnes without pain')._.outcome_spellCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>source_clean</th>\n",
       "      <th>python_code_names</th>\n",
       "      <th>code_comments</th>\n",
       "      <th>comments_entities</th>\n",
       "      <th>code_comments_lemm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>cell_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">000e671b3324e2</th>\n",
       "      <th>17857a56</th>\n",
       "      <td>code</td>\n",
       "      <td># construct train input\\n\\noutput = pd.DataFrame()\\ngb = train.groupby('Patient')\\ntk0 = tqdm(gb, total=len(gb))\\nfo...</td>\n",
       "      <td>construct train input output pd dataframe gb train groupby reset index drop true print train shape train head</td>\n",
       "      <td>output pd DataFrame gb train groupby tk0 tqdm gb total len gb for _ usr_df in tk0 usr_output pd DataFrame for week t...</td>\n",
       "      <td>construct train input</td>\n",
       "      <td></td>\n",
       "      <td>construct train input</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fba07254</th>\n",
       "      <td>code</td>\n",
       "      <td># construct test input\\n\\ntest = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/test.csv')\\\\n        .ren...</td>\n",
       "      <td>construct test input test pd read csv print test shape test head</td>\n",
       "      <td>test pd read_csv rename columns submission pd read_csv submission submission apply lambda x x split submission submi...</td>\n",
       "      <td>construct test input</td>\n",
       "      <td></td>\n",
       "      <td>construct test input</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffb99051</th>\n",
       "      <td>code</td>\n",
       "      <td>folds = train[[ID, 'Patient', TARGET]].copy()\\n#Fold = KFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)\\nFold...</td>\n",
       "      <td>folds train id astype int folds head</td>\n",
       "      <td>folds train ID TARGET copy Fold GroupKFold n_splits N_FOLD groups folds values for n train_index val_index in enumer...</td>\n",
       "      <td>fold kfoldnsplitsnfold shuffletrue randomstateseed</td>\n",
       "      <td>ORG</td>\n",
       "      <td>fold kfoldnsplitsnfold shuffletrue randomstateseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2afe8a24</th>\n",
       "      <td>code</td>\n",
       "      <td>#===========================================================\\n# model\\n#============================================...</td>\n",
       "      <td>model from sklearn preprocessing import minmaxscaler scaler minmaxscaler def run single ridge param train df test df...</td>\n",
       "      <td>from sklearn preprocessing import MinMaxScaler scaler MinMaxScaler def run_single_ridge param train_df test_df folds...</td>\n",
       "      <td>model xtr scalerfittransformxtr xval scalertransformxval</td>\n",
       "      <td></td>\n",
       "      <td>model xtr scalerfittransformxtr xval scalertransformxval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d349aeae</th>\n",
       "      <td>code</td>\n",
       "      <td>target = train[TARGET]\\ntest[TARGET] = np.nan\\n\\n# features\\ncat_features = ['Sex', 'SmokingStatus']\\nnum_features =...</td>\n",
       "      <td>target train target test target np nan features cat features seed oof predictions run kfold ridge ridge param train ...</td>\n",
       "      <td>target train TARGET test TARGET np nan cat_features num_features c for c in test columns if test dtypes c c not in c...</td>\n",
       "      <td>features</td>\n",
       "      <td></td>\n",
       "      <td>feature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ffccaad7a9a626</th>\n",
       "      <th>85134a3b</th>\n",
       "      <td>code</td>\n",
       "      <td># Dimensions of te Data\\n\\niris.shape</td>\n",
       "      <td>dimensions of te data iris shape</td>\n",
       "      <td>iris shape</td>\n",
       "      <td>dimensions of te data</td>\n",
       "      <td>ORG</td>\n",
       "      <td>dimension of te data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dae3e97d</th>\n",
       "      <td>code</td>\n",
       "      <td># Statistical summary of all attributes\\n\\niris.describe()</td>\n",
       "      <td>statistical summary of all attributes iris describe</td>\n",
       "      <td>iris describe</td>\n",
       "      <td>statistical summary of all attributes</td>\n",
       "      <td></td>\n",
       "      <td>statistical summary of all attribute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c698528a</th>\n",
       "      <td>code</td>\n",
       "      <td># To know how many variety of species are present in the data\\n\\niris.groupby('Species').size()</td>\n",
       "      <td>to know how many variety of species are present in the data iris groupby size</td>\n",
       "      <td>iris groupby size</td>\n",
       "      <td>to know how many variety of species are present in the data</td>\n",
       "      <td></td>\n",
       "      <td>to know how many variety of specie are present in the data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4d5ac628</th>\n",
       "      <td>code</td>\n",
       "      <td># Selecting Dependent and Independent Variables\\nX = iris.iloc[:, [1, 3]].values\\ny = iris.iloc[:, -1].values</td>\n",
       "      <td>selecting dependent and independent variables x iris iloc values y iris iloc 1 values</td>\n",
       "      <td>X iris iloc values y iris iloc values</td>\n",
       "      <td>selecting dependent and independent variables</td>\n",
       "      <td></td>\n",
       "      <td>selecting dependent and independent variable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9b479da1</th>\n",
       "      <td>code</td>\n",
       "      <td># Splitting into training and test sets\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_tr...</td>\n",
       "      <td>splitting into training and test sets from sklearn model selection import train test split x train x test y train y ...</td>\n",
       "      <td>from sklearn model_selection import train_test_split X_train X_test y_train y_test train_test_split X y test_size ra...</td>\n",
       "      <td>splitting into training and test sets</td>\n",
       "      <td></td>\n",
       "      <td>splitting into training and test set</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9496 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        cell_type                                                                                                                   source  \\\n",
       "id             cell_id                                                                                                                                       \n",
       "000e671b3324e2 17857a56      code  # construct train input\\n\\noutput = pd.DataFrame()\\ngb = train.groupby('Patient')\\ntk0 = tqdm(gb, total=len(gb))\\nfo...   \n",
       "               fba07254      code  # construct test input\\n\\ntest = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/test.csv')\\\\n        .ren...   \n",
       "               ffb99051      code  folds = train[[ID, 'Patient', TARGET]].copy()\\n#Fold = KFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)\\nFold...   \n",
       "               2afe8a24      code  #===========================================================\\n# model\\n#============================================...   \n",
       "               d349aeae      code  target = train[TARGET]\\ntest[TARGET] = np.nan\\n\\n# features\\ncat_features = ['Sex', 'SmokingStatus']\\nnum_features =...   \n",
       "...                           ...                                                                                                                      ...   \n",
       "ffccaad7a9a626 85134a3b      code                                                                                    # Dimensions of te Data\\n\\niris.shape   \n",
       "               dae3e97d      code                                                               # Statistical summary of all attributes\\n\\niris.describe()   \n",
       "               c698528a      code                          # To know how many variety of species are present in the data\\n\\niris.groupby('Species').size()   \n",
       "               4d5ac628      code            # Selecting Dependent and Independent Variables\\nX = iris.iloc[:, [1, 3]].values\\ny = iris.iloc[:, -1].values   \n",
       "               9b479da1      code  # Splitting into training and test sets\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_tr...   \n",
       "\n",
       "                                                                                                                                    source_clean  \\\n",
       "id             cell_id                                                                                                                             \n",
       "000e671b3324e2 17857a56            construct train input output pd dataframe gb train groupby reset index drop true print train shape train head   \n",
       "               fba07254                                                         construct test input test pd read csv print test shape test head   \n",
       "               ffb99051                                                                                     folds train id astype int folds head   \n",
       "               2afe8a24  model from sklearn preprocessing import minmaxscaler scaler minmaxscaler def run single ridge param train df test df...   \n",
       "               d349aeae  target train target test target np nan features cat features seed oof predictions run kfold ridge ridge param train ...   \n",
       "...                                                                                                                                          ...   \n",
       "ffccaad7a9a626 85134a3b                                                                                         dimensions of te data iris shape   \n",
       "               dae3e97d                                                                      statistical summary of all attributes iris describe   \n",
       "               c698528a                                            to know how many variety of species are present in the data iris groupby size   \n",
       "               4d5ac628                                    selecting dependent and independent variables x iris iloc values y iris iloc 1 values   \n",
       "               9b479da1  splitting into training and test sets from sklearn model selection import train test split x train x test y train y ...   \n",
       "\n",
       "                                                                                                                               python_code_names  \\\n",
       "id             cell_id                                                                                                                             \n",
       "000e671b3324e2 17857a56  output pd DataFrame gb train groupby tk0 tqdm gb total len gb for _ usr_df in tk0 usr_output pd DataFrame for week t...   \n",
       "               fba07254  test pd read_csv rename columns submission pd read_csv submission submission apply lambda x x split submission submi...   \n",
       "               ffb99051  folds train ID TARGET copy Fold GroupKFold n_splits N_FOLD groups folds values for n train_index val_index in enumer...   \n",
       "               2afe8a24  from sklearn preprocessing import MinMaxScaler scaler MinMaxScaler def run_single_ridge param train_df test_df folds...   \n",
       "               d349aeae  target train TARGET test TARGET np nan cat_features num_features c for c in test columns if test dtypes c c not in c...   \n",
       "...                                                                                                                                          ...   \n",
       "ffccaad7a9a626 85134a3b                                                                                                               iris shape   \n",
       "               dae3e97d                                                                                                            iris describe   \n",
       "               c698528a                                                                                                        iris groupby size   \n",
       "               4d5ac628                                                                                    X iris iloc values y iris iloc values   \n",
       "               9b479da1  from sklearn model_selection import train_test_split X_train X_test y_train y_test train_test_split X y test_size ra...   \n",
       "\n",
       "                                                                       code_comments comments_entities                                          code_comments_lemm  \n",
       "id             cell_id                                                                                                                                              \n",
       "000e671b3324e2 17857a56                                        construct train input                                                         construct train input  \n",
       "               fba07254                                         construct test input                                                          construct test input  \n",
       "               ffb99051           fold kfoldnsplitsnfold shuffletrue randomstateseed               ORG          fold kfoldnsplitsnfold shuffletrue randomstateseed  \n",
       "               2afe8a24     model xtr scalerfittransformxtr xval scalertransformxval                      model xtr scalerfittransformxtr xval scalertransformxval  \n",
       "               d349aeae                                                     features                                                                       feature  \n",
       "...                                                                              ...               ...                                                         ...  \n",
       "ffccaad7a9a626 85134a3b                                        dimensions of te data               ORG                                        dimension of te data  \n",
       "               dae3e97d                        statistical summary of all attributes                                          statistical summary of all attribute  \n",
       "               c698528a  to know how many variety of species are present in the data                    to know how many variety of specie are present in the data  \n",
       "               4d5ac628                selecting dependent and independent variables                                  selecting dependent and independent variable  \n",
       "               9b479da1                        splitting into training and test sets                                          splitting into training and test set  \n",
       "\n",
       "[9496 rows x 7 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result['code_comments']!='']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## markdowns pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_pipeline = Pipeline([\n",
    "    ('Clearing and parsing source', FunctionTransformer(\n",
    "         func=prepare_data,\n",
    "         kw_args={\n",
    "             'func': code_preprocessing,\n",
    "             'new_column': \"source_clean\",\n",
    "             'on_column': \"source\"})),\n",
    "    ('Lemmatizing sentences', FunctionTransformer(\n",
    "         func=prepare_data,\n",
    "         kw_args={\n",
    "             'func': code_preprocessing,\n",
    "             'new_column': \"source_clean\",\n",
    "             'on_column': \"source\"})),\n",
    "    \n",
    "\n",
    "], verbose=True)\n",
    "result = code_pipeline.fit_transform(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([codes, markdowns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# counting features\n",
    "df['text_len'] = df['source_clean'].astype(str).apply(len)\n",
    "df['text_word_count'] = df['source_clean'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO make the right order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning glossary feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_glossary = pd.read_csv(\"machine_learning_glossary_terms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences we will be searching through\n",
    "corpus = np.array(ml_glossary['definition'])\n",
    "terms = np.array(ml_glossary['term'])\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### implementing feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better do not vector but words and the put them to TFID\n",
    "top_k=7\n",
    "def get_top_glossary_terms(query):\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)\n",
    "    hits = ['_'.join(str(terms[hit['corpus_id']]).split()) for hit in hits[0]]\n",
    "    return ' '.join(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c840ca7f2160441f96eb686538d73f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=47329.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df.loc[:, 'glossary_ml_terms'] = (\n",
    "    df['source_clean'].apply(str).progress_apply(lambda x: get_top_glossary_terms(x)).copy().values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column not found: rank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-e40666ff7826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pct_rank'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cell_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rank\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.20\u001b[0m\u001b[0;36m.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1613\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1614\u001b[0m         \u001b[0mParameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m         \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selection_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexclusions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexclusions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Column not found: rank'"
     ]
    }
   ],
   "source": [
    "df['pct_rank'] = df.groupby([\"id\", \"cell_type\"])[\"rank\"].apply(lambda s: pd.Series((np.arange(len(s)) + 1) /(len(s) + 1), index=s.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_each_cell_with_its_position(current_X, full_df):\n",
    "    \"\"\"\n",
    "    marking each cell with its number if its code, for markdown zero. \n",
    "    We are doing it to help the model learn the correct order in lines?\n",
    "    \"\"\"\n",
    "    old_shape = current_X.shape\n",
    "    current_X = sparse.hstack((\n",
    "        current_X,\n",
    "        np.where(\n",
    "            full_df['cell_type'] == 'code',\n",
    "            full_df.groupby(['id', 'cell_type']).cumcount().to_numpy() + 1,\n",
    "            0,\n",
    "        ).reshape(-1, 1)\n",
    "    ))\n",
    "    new_shape = current_X.shape\n",
    "    print(f\"cell with position change {old_shape} -> {new_shape}\")\n",
    "    return current_X\n",
    "\n",
    "# idf(t) = log [ n / df(t) ] + 1, where df(t) – number of time term is used\n",
    "tfidf = TfidfVectorizer(min_df=0.01, max_features=169)\n",
    "def convert_to_TfidfVector(df):\n",
    "    print(\"Converting with Tfid vectorizer\")\n",
    "    return tfidf.fit_transform(df.astype(str))\n",
    "\n",
    "def add_data_to_sparse(current_X, values):\n",
    "    print(f\"Added {values.shape} to the dataframe\")\n",
    "    return sparse.hstack((\n",
    "        current_X,\n",
    "        values\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
