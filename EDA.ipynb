{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c029aca-0a59-4b10-8462-a3e8a0a77925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f26b2e8-c9ff-4e5f-a67a-5b452140a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"data/glove/glove.6B.100d.txt\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        token = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[token] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9266fe63-9e8a-4b18-8945-fc24a0055ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_embeddings(embedding, cutoff=25):\n",
    "    return sorted(embeddings_dict.keys(), key=lambda token: spatial.distance.euclidean(embeddings_dict[token], embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c5ca0e4-72e0-4ac6-8525-d5d211f343fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flashlight', 'twig', 'clipboard', 'shove', 'hand']\n"
     ]
    }
   ],
   "source": [
    "print(find_closest_embeddings(\n",
    "    embeddings_dict[\"twig\"] - embeddings_dict[\"branch\"] + embeddings_dict[\"hand\"]\n",
    ")[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "90189727-a693-4671-a43c-c2068271a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_a = \"Here we perform 5-fold cross validation of a KNN model after using a standard scaler\"\n",
    "sentence_b = \"In this kernel I present a very simple K-nearest neighbors model based on the quantiles of the distribution\"\n",
    "sentence_c = \"And so it begins but you can't have them all. Your heart has to settle down somewhere\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "974da328-caeb-4e3e-9612-2489fb50cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_a = np.zeros_like(embeddings_dict[\"branch\"])\n",
    "embedding_b = np.zeros_like(embeddings_dict[\"branch\"])\n",
    "embedding_c = np.zeros_like(embeddings_dict[\"branch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e41d21ad-afab-48ac-b4cb-9f8630ea50a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in sentence_a.split():\n",
    "    if w not in embeddings_dict:\n",
    "        continue\n",
    "    embedding_a += embeddings_dict[w]\n",
    "\n",
    "for w in sentence_b.split():\n",
    "    if w not in embeddings_dict:\n",
    "        continue\n",
    "    embedding_b += embeddings_dict[w]\n",
    "\n",
    "for w in sentence_c.split():\n",
    "    if w not in embeddings_dict:\n",
    "        continue\n",
    "    embedding_c += embeddings_dict[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "978f38ce-15d7-46d9-8505-65288976c0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'an', 'this', 'be', 'for']\n"
     ]
    }
   ],
   "source": [
    "print(find_closest_embeddings(\n",
    "    embedding_a\n",
    ")[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "71ed5c0b-293f-4535-8e75-5545b27dc012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.454050064086914"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatial.distance.euclidean(embedding_a, embedding_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de61e1ab-19ec-433d-8c12-9e71203feb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.27524185180664"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatial.distance.euclidean(embedding_a, embedding_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc70ebaf-3ab3-4c10-b4ac-967aae41047c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.673566818237305"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatial.distance.euclidean(embedding_b, embedding_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa96ced0-72d5-45a4-9a69-dd7776c989b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial.distance.euclidean(embeddings_dict[token], embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce4169-5f91-453c-bb8a-34d4c729f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "return sorted(embeddings_dict.keys(), key=lambda token: spatial.distance.euclidean(embeddings_dict[token], embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda7ec2-8148-4398-8f62-72c85479be3c",
   "metadata": {},
   "source": [
    "# bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20419cb0-c55e-4276-9ead-9c3fa0d4be65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence-transformers-2.2.1.tar.gz (84 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.42.1)\n",
      "Collecting torch>=1.6.0\n"
     ]
    }
   ],
   "source": [
    "! pip install -U sentence-transformers\n",
    "# https://www.sbert.net/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8571db54-8a82-4efd-a53b-a165980510f1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torch-1.11.0%2Bcpu-cp37-cp37m-linux_x86_64.whl (169.1 MB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torchvision-0.12.0%2Bcpu-cp37-cp37m-linux_x86_64.whl (14.7 MB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torchaudio-0.11.0%2Bcpu-cp37-cp37m-linux_x86_64.whl (2.7 MB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.2.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision) (2.28.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision) (9.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (1.26.9)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-1.11.0+cpu torchaudio-0.11.0+cpu torchvision-0.12.0+cpu\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46e8479a-ede1-4d7a-b224-6fd652156d18",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lexrank\n",
      "  Downloading lexrank-0.1.0-py3-none-any.whl (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.8/69.8 kB\u001b[0m \u001b[31m933.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting urlextract>=0.7\n",
      "  Downloading urlextract-1.6.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from lexrank) (1.21.6)\n",
      "Collecting path.py>=10.5\n",
      "  Downloading path.py-12.5.0-py3-none-any.whl (2.3 kB)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /opt/conda/lib/python3.7/site-packages (from lexrank) (1.4.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from lexrank) (0.15.7)\n",
      "Requirement already satisfied: regex>=2017.11.9 in /opt/conda/lib/python3.7/site-packages (from lexrank) (2022.6.2)\n",
      "Requirement already satisfied: path in /opt/conda/lib/python3.7/site-packages (from path.py>=10.5->lexrank) (13.1.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from pyrsistent>=0.14.0->lexrank) (1.14.0)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.7/site-packages (from urlextract>=0.7->lexrank) (2.8)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.7/site-packages (from urlextract>=0.7->lexrank) (2.5.2)\n",
      "Collecting uritools\n",
      "  Downloading uritools-4.0.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from urlextract>=0.7->lexrank) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata>=0.5 in /opt/conda/lib/python3.7/site-packages (from path->path.py>=10.5->lexrank) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.5->path->path.py>=10.5->lexrank) (2.2.0)\n",
      "Installing collected packages: uritools, urlextract, path.py, lexrank\n",
      "Successfully installed lexrank-0.1.0 path.py-12.5.0 uritools-4.0.0 urlextract-1.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "! pip install lexrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d59b4d78-bc4f-455e-9ba2-ebeb4aa7fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "import pylev\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc022c6c-a6b7-426f-b42c-f8a1ccd90ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LexRank implementation\n",
    "Source: https://github.com/crabcamp/lexrank/tree/dev\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from scipy.special import softmax\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def degree_centrality_scores(\n",
    "    similarity_matrix,\n",
    "    threshold=None,\n",
    "    increase_power=True,\n",
    "):\n",
    "    if not (\n",
    "        threshold is None\n",
    "        or isinstance(threshold, float)\n",
    "        and 0 <= threshold < 1\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            '\\'threshold\\' should be a floating-point number '\n",
    "            'from the interval [0, 1) or None',\n",
    "        )\n",
    "\n",
    "    if threshold is None:\n",
    "        markov_matrix = create_markov_matrix(similarity_matrix)\n",
    "\n",
    "    else:\n",
    "        markov_matrix = create_markov_matrix_discrete(\n",
    "            similarity_matrix,\n",
    "            threshold,\n",
    "        )\n",
    "\n",
    "    scores = stationary_distribution(\n",
    "        markov_matrix,\n",
    "        increase_power=increase_power,\n",
    "        normalized=False,\n",
    "    )\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def _power_method(transition_matrix, increase_power=True, max_iter=10000):\n",
    "    eigenvector = np.ones(len(transition_matrix))\n",
    "\n",
    "    if len(eigenvector) == 1:\n",
    "        return eigenvector\n",
    "\n",
    "    transition = transition_matrix.transpose()\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        eigenvector_next = np.dot(transition, eigenvector)\n",
    "\n",
    "        if np.allclose(eigenvector_next, eigenvector):\n",
    "            return eigenvector_next\n",
    "\n",
    "        eigenvector = eigenvector_next\n",
    "\n",
    "        if increase_power:\n",
    "            transition = np.dot(transition, transition)\n",
    "\n",
    "    logger.warning(\"Maximum number of iterations for power method exceeded without convergence!\")\n",
    "    return eigenvector_next\n",
    "\n",
    "\n",
    "def connected_nodes(matrix):\n",
    "    _, labels = connected_components(matrix)\n",
    "\n",
    "    groups = []\n",
    "\n",
    "    for tag in np.unique(labels):\n",
    "        group = np.where(labels == tag)[0]\n",
    "        groups.append(group)\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def create_markov_matrix(weights_matrix):\n",
    "    n_1, n_2 = weights_matrix.shape\n",
    "    if n_1 != n_2:\n",
    "        raise ValueError('\\'weights_matrix\\' should be square')\n",
    "\n",
    "    row_sum = weights_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # normalize probability distribution differently if we have negative transition values\n",
    "    if np.min(weights_matrix) <= 0:\n",
    "        return softmax(weights_matrix, axis=1)\n",
    "\n",
    "    return weights_matrix / row_sum\n",
    "\n",
    "\n",
    "def create_markov_matrix_discrete(weights_matrix, threshold):\n",
    "    discrete_weights_matrix = np.zeros(weights_matrix.shape)\n",
    "    ixs = np.where(weights_matrix >= threshold)\n",
    "    discrete_weights_matrix[ixs] = 1\n",
    "\n",
    "    return create_markov_matrix(discrete_weights_matrix)\n",
    "\n",
    "\n",
    "def stationary_distribution(\n",
    "    transition_matrix,\n",
    "    increase_power=True,\n",
    "    normalized=True,\n",
    "):\n",
    "    n_1, n_2 = transition_matrix.shape\n",
    "    if n_1 != n_2:\n",
    "        raise ValueError('\\'transition_matrix\\' should be square')\n",
    "\n",
    "    distribution = np.zeros(n_1)\n",
    "\n",
    "    grouped_indices = connected_nodes(transition_matrix)\n",
    "\n",
    "    for group in grouped_indices:\n",
    "        t_matrix = transition_matrix[np.ix_(group, group)]\n",
    "        eigenvector = _power_method(t_matrix, increase_power=increase_power)\n",
    "        distribution[group] = eigenvector\n",
    "\n",
    "    if normalized:\n",
    "        distribution /= n_1\n",
    "\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dcb98de-5c39-45ad-ab6f-d50ee4fc4672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sentences: 29\n",
      "\n",
      "\n",
      "Summary:\n",
      "New York City (NYC), often called simply New York, is the most populous city in the United States.\n",
      "In 2019, New York was voted the greatest city in the world per a survey of over 30,000 people from 48 cities worldwide, citing its cultural diversity.\n",
      "Located at the southern tip of the U.S. state of New York, the city is the center of the New York metropolitan area, the largest metropolitan area in the world by urban landmass.\n",
      "With an estimated 2019 population of 8,336,817 distributed over about 302.6 square miles (784 km2), New York City is also the most densely populated major city in the United States.\n",
      "New York City was the capital of the United States from 1785 until 1790, and has been the largest U.S. city since 1790.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Our input document we want to summarize\n",
    "# As example, we take the first section from Wikipedia\n",
    "document = \"\"\"\n",
    "New York City (NYC), often called simply New York, is the most populous city in the United States. With an estimated 2019 population of 8,336,817 distributed over about 302.6 square miles (784 km2), New York City is also the most densely populated major city in the United States. Located at the southern tip of the U.S. state of New York, the city is the center of the New York metropolitan area, the largest metropolitan area in the world by urban landmass. With almost 20 million people in its metropolitan statistical area and approximately 23 million in its combined statistical area, it is one of the world's most populous megacities. New York City has been described as the cultural, financial, and media capital of the world, significantly influencing commerce, entertainment, research, technology, education, politics, tourism, art, fashion, and sports. Home to the headquarters of the United Nations, New York is an important center for international diplomacy.\n",
    "Situated on one of the world's largest natural harbors, New York City is composed of five boroughs, each of which is a county of the State of New York. The five boroughs—Brooklyn, Queens, Manhattan, the Bronx, and Staten Island—were consolidated into a single city in 1898. The city and its metropolitan area constitute the premier gateway for legal immigration to the United States. As many as 800 languages are spoken in New York, making it the most linguistically diverse city in the world. New York is home to more than 3.2 million residents born outside the United States, the largest foreign-born population of any city in the world as of 2016. As of 2019, the New York metropolitan area is estimated to produce a gross metropolitan product (GMP) of $2.0 trillion. If the New York metropolitan area were a sovereign state, it would have the eighth-largest economy in the world. New York is home to the highest number of billionaires of any city in the world.\n",
    "New York City traces its origins to a trading post founded by colonists from the Dutch Republic in 1624 on Lower Manhattan; the post was named New Amsterdam in 1626. The city and its surroundings came under English control in 1664 and were renamed New York after King Charles II of England granted the lands to his brother, the Duke of York. The city was regained by the Dutch in July 1673 and was subsequently renamed New Orange for one year and three months; the city has been continuously named New York since November 1674. New York City was the capital of the United States from 1785 until 1790, and has been the largest U.S. city since 1790. The Statue of Liberty greeted millions of immigrants as they came to the U.S. by ship in the late 19th and early 20th centuries, and is a symbol of the U.S. and its ideals of liberty and peace. In the 21st century, New York has emerged as a global node of creativity, entrepreneurship, and environmental sustainability, and as a symbol of freedom and cultural diversity. In 2019, New York was voted the greatest city in the world per a survey of over 30,000 people from 48 cities worldwide, citing its cultural diversity.\n",
    "Many districts and landmarks in New York City are well known, including three of the world's ten most visited tourist attractions in 2013. A record 62.8 million tourists visited New York City in 2017. Times Square is the brightly illuminated hub of the Broadway Theater District, one of the world's busiest pedestrian intersections, and a major center of the world's entertainment industry. Many of the city's landmarks, skyscrapers, and parks are known around the world. Manhattan's real estate market is among the most expensive in the world. Providing continuous 24/7 service and contributing to the nickname The City that Never Sleeps, the New York City Subway is the largest single-operator rapid transit system worldwide, with 472 rail stations. The city has over 120 colleges and universities, including Columbia University, New York University, Rockefeller University, and the City University of New York system, which is the largest urban public university system in the United States. Anchored by Wall Street in the Financial District of Lower Manhattan, New York City has been called both the world's leading financial center and the most financially powerful city in the world, and is home to the world's two largest stock exchanges by total market capitalization, the New York Stock Exchange and NASDAQ.\n",
    "\"\"\"\n",
    "\n",
    "#Split the document into sentences\n",
    "sentences = nltk.sent_tokenize(document)\n",
    "print(\"Num sentences:\", len(sentences))\n",
    "\n",
    "#Compute the sentence embeddings\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "#Compute the pair-wise cosine similarities\n",
    "cos_scores = util.cos_sim(embeddings, embeddings).numpy()\n",
    "\n",
    "#Compute the centrality for each sentence\n",
    "centrality_scores = degree_centrality_scores(cos_scores, threshold=None)\n",
    "\n",
    "#We argsort so that the first element is the sentence with the highest score\n",
    "most_central_sentence_indices = np.argsort(-centrality_scores)\n",
    "\n",
    "\n",
    "#Print the 5 sentences with the highest scores\n",
    "print(\"\\n\\nSummary:\")\n",
    "for idx in most_central_sentence_indices[0:5]:\n",
    "    print(sentences[idx].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d356f2de-4575-4fc6-aa84-532591583a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_data(data_dir, NUM_TRAIN = 10000):\n",
    "    def read_notebook(path):\n",
    "        return (\n",
    "            pd.read_json(\n",
    "                path,\n",
    "                dtype={'cell_type': 'category', 'source': 'str'})\n",
    "            .assign(id=path.stem)  # final path component\n",
    "            .rename_axis('cell_id')\n",
    "        )\n",
    "\n",
    "    paths_train = list((data_dir / 'train').glob('*.json'))[:NUM_TRAIN]\n",
    "    notebooks_train = [\n",
    "      read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')\n",
    "    ]\n",
    "    df = (\n",
    "      pd.concat(notebooks_train)\n",
    "      .set_index('id', append=True)\n",
    "      .swaplevel()\n",
    "      .sort_index(level='id', sort_remaining=False)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a7ec400-0db4-4873-b6c8-5064f4539c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "145f5592-f54a-400f-b69a-a51df3533604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train NBs: 100%|██████████| 10000/10000 [02:35<00:00, 64.31it/s]\n"
     ]
    }
   ],
   "source": [
    "df = read_train_data(data_dir, NUM_TRAIN=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e226537-6720-4c09-b92d-234d10db4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index()."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
