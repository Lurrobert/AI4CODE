term,definition
validation,"An optional final stage of model building in which the refined
model from the testing stage is validated against a further subset
of the source data. See also model building, testing,
training."
unrefined model,"A model that contains information extracted from the data but
which is not designed for generating predictions directly."
transformation,"A formula that is applied to the values of a field to alter the
distribution of values. Some statistical methods require that
fields have a particular distribution. When a field's distribution
differs from what is required, a transformation (such as taking
logarithms of values) can often remedy the problem."
training,"The initial stage of model building, involving a subset of the
source data. The model can then be tested against a further,
different subset for which the outcome is already known. See also
model building, testing, validation."
testing,"The stage of model building in which the model produced by the
training stage is tested against a data subset for which the
outcome is already known. See also model building, training,
validation."
script,"A series of commands, combined in a file, that carry out a
particular function when the file is run. Scripts are interpreted
as they are run."
score,"To apply a predictive model to a data set with the intention of
producing a classification or prediction for a new, untested
case."
regression tree algorithm,"A tree-based algorithm that splits a sample of cases repeatedly
to derive homogeneous subsets, based on values of a numeric output
field. See also Chi-squared Automatic Interaction Detector
algorithm."
regression,"A statistical technique for estimating the value of a target
field based on the values of one or more input fields. See also
linear regression, logistic regression."
"Quick, Unbiased, Efficient Statistical Tree algorithm
(QUEST)","A decision tree algorithm that provides a binary classification
method for building the tree. The algorithm is designed to reduce
the processing time required for large C & R tree analyses
while also reducing the tendency found in classification tree
methods to favor inputs that allow more splits. See also
classification and regression tree algorithm, decision tree
algorithm."
probability,"A measure of the likelihood that an event will occur.
Probability values range from 0 to 1; 0 implies that the event
never occurs, and 1 implies that the event always occurs. A
probability of 0.5 indicates that the event has an even chance of
occurring or not occurring."
Predictive Model Markup Language (PMML),"An XML-based language defined by the Data Mining Group that
provides a way for companies to define predictive models and share
models between compliant vendors' applications."
predictive analytics,"A business process and a set of related technologies that are
concerned with the prediction of future possibilities and trends.
Predictive analytics applies such diverse disciplines as
probability, statistics, machine learning, and artificial
intelligence to business problems to find the best action for a
given situation."
partition,"To divide a data set into separate subsets or samples for the
training, testing, and validation stages of model building."
online scoring,"Apply model prediction real time on a single record through a
published endpoint within or outside the organization, expects fast
response in terms of milliseconds"
neural network,"A mathematical model for predicting or classifying cases by
using a complex mathematical scheme that simulates an abstract
version of brain cells. A neural network is trained by presenting
it with a large number of observed cases, one at a time, and
allowing it to update itself repeatedly until it learns the
task."
multinomial logistic regression,"A logistic regression that is used for targets with more than
two categories. See also binomial logistic regression, target."
model building,"The process of creating data models by using algorithms. Model
building typically consists of several stages: training, testing
and (optionally) validation of evaluation. See also testing,
training, validation."
misclassification cost,"A specification of the relative importance of different kinds
of classification errors, such as classifying a high-risk credit
applicant as low risk. Costs are specified in the form of weights
applied to specific incorrect predictions."
logistic regression,"A statistical technique for classifying records based on the
values of the input fields. Logistic regression is similar to
linear regression, but takes a categorical target field instead of
a numeric one. See also regression."
linear regression model,"A modeling algorithm that assumes that the relationship between
the input and the output for the model is of a particular, simple
form. The model fits the best line through linear regression and
generates a linear mapping between the input variables and each
output variable."
linear regression,"A statistical technique for estimating a linear model for a
continuous (numeric) output field. Linear models predict a
continuous target based on linear relationships between the target
and one or more predictors. See also regression."
histogram,"A graphical display of the distribution of values for a numeric
field, in the form of a vertical bar chart in which taller bars
indicate higher values."
heat map,"A graphical representation of data values in a two-dimensional
table format, in which higher values are represented by darker
colors and lower values by lighter ones."
evaluate,"The process of determining whether a model will accurately
predict the target on new and future data."
decision tree algorithm,"An algorithm that classifies data, or predicts future outcomes,
based on a set of decision rules."
decision list,"An algorithm that identifies subgroups or segments that show a
higher or lower likelihood of a given binary (yes/no) outcome
relative to the overall population."
data visualization,"The process of presenting data patterns in graphical format,
including the use of traditional plots as well as advanced
interactive graphics. In many cases, visualization reveals patterns
that would be difficult to find using other methods."
data set,"A collection of data, usually in the form of rows (records) and
columns (fields) and contained in a file or database table."
data quality,"The extent to which data has been accurately coded and stored.
Factors that adversely affect data quality include missing values,
data entry errors, measurement errors, and coding
inconsistencies."
cross-validation,"A technique for testing how well a model generalizes in the
absence of a holdout test sample. Cross-validation divides the
training data into a number of subsets, and then builds the same
number of models, with each subset held out in turn. Each of those
models is tested on the holdout sample, and the average accuracy of
the models on those holdout samples is used to estimate the
accuracy of the model when applied to new data. See also
overfitting."
Cox regression algorithm,"An algorithm that produces a survival function that predicts
the probability that the event of interest has occurred at a given
time for given values of the predictor variables."
confidence score,"An estimate of the accuracy of a prediction, usually expressed
as a number from 0.0 to 1.0."
classification and regression tree algorithm,"A decision tree algorithm that uses recursive partitioning to
split the training records into segments by minimizing the impurity
at each step. See also Quick, Unbiased, Efficient Statistical Tree
algorithm."
binomial logistic regression,"A logistic regression that is used for targets with two
discrete categories. See also multinomial logistic regression,
target."
Bayesian network,"A graphical model that displays variables in a data set and the
probabilistic or conditional in-dependencies between them."
batch scoring,"running the the model predictions offline (asynchronously) on a
large dataset."
association,"The extent to which values of one field depend on or are
predicted by values of another field."
accuracy,Accuracy is a metric by which one can examine how good is the machine learning model. Let us look at the confusion matrix to understand it in a better way:
adam_optimization,"The Adam Optimization algorithm is used in training deep learning models. It is an extension to Stochastic Gradient Descent. In this optimization algorithm, running averages of both the gradients and the second moments of the gradients are used. It is used to compute adaptive learning rates for each parameter."
apache_spark,"Apache Spark is an open-source cluster computing framework. Spark can be deployed in a variety of ways, provides native bindings for the Java, Scala, Python, and R programming languages, and supports SQL, streaming data, and machine learning. Some of the key features of Apache Spark are listed below:"
backpropogation,"In neural networks, if the estimated output is far away from the actual output (high error), we update the biases and weights based on the error. This weight and bias updating process is known as Back Propagation. Back-propagation (BP) algorithms work by determining the loss (or error) at the output and then propagating it back into the network. The weights are updated to minimize the error resulting from each neuron. The first step in minimizing the error is to determine the gradient (Derivatives) of each node w.r.t. the final output."
bagging,"Bagging or bootstrap averaging is a technique where multiple models are created on the subset of data, and the final predictions are determined by combining the predictions of all the models. Some of the algorithms that use bagging technique are :"
bar_chart,"Bar charts are a type of graph that are used to display and compare the numbers, frequency or other measures (e.g. mean) for different discrete categories of data. They are used for categorical variables. Simple example of a bar chart:"
bayesian_statistics,"Bayesian statistics is a mathematical procedure that applies probabilities to statistical problems. It provides people the tools to update their beliefs in the evidence of new data. It differs from classical frequentist approach and is based on the use of Bayesian probabilities to summarize evidence. For more details, read"
bias_variance_trade_off,The error emerging from any model can be broken down into components mathematically.
big_data,"Big data is a term that describes the large volume of data – both structured and unstructured. But it’s not the amount of data that’s important. It’s how organizations use this large amount of data to generate insights. Companies use various tools, techniques and resources to make sense of this data to derive effective business strategies."
binary_variable,"Binary variables are those variables which can have only two unique values. For example, a variable “Smoking Habit” can contain only two values like “Yes” and “No”."
boosting,"Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model."
bootstrapping,"Bootstrapping is the process of dividing the dataset into multiple subsets, with replacement. Each subset is of the same size of the dataset. These samples are called bootstrap samples."
box_plot,"It displays the full range of variation (from min to max), the"
business_analytics,Business analytics is mainly used to show the practical methodology followed by an organization for exploring data to gain insights. The methodology focusses on statistical analysis of the data.
business_intelligence,"Business intelligence are a set of strategies, applications, data, technologies used by an organization for data collection, analysis and generating insights to derive strategic business opportunities."
autoregression,"Autoregression is a time series model that uses observations from previous time steps as input to a regression equation to predict the value at the next time step. The autoregressive model specifies that the output variable depends linearly on its own previous values. In this technique input variables are taken as observations at previous time steps, called lag variables."
computer_vision,"Computer Vision is a field of computer science that deals with enabling computers to visualize, process and identify images/videos in the same way that a human vision does. In the recent times, the major driving forces behind Computer Vision has been the emergence of deep learning, rise in computational power and a huge amount of image data. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. Some of the key applications of Computer Vision are:"
concordant_discordant_ratio,"Concordant and discordant pairs are used to describe the relationship between pairs of observations. To calculate the concordant and discordant pairs, the data are treated as ordinal. The number of concordant and discordant pairs are used in calculations for Kendall’s tau, which measures the association between two ordinal variables."
confidence_interval,"A confidence interval is used to estimate what percent of a population fits a category based on the results from a sample population. For example, if 70 adults own a cell phone in a random sample of 100 adults, we can be fairly confident that the true percentage amongst the population is somewhere between 61% and 79%. Read more"
continuous_variable,"Continuous variables are those variables which can have infinite number of values but only in a specific range. For example, height is a continuous variable. Read more"
convergence,Convergence refers to moving towards union or uniformity. An iterative algorithm is said to converge when as the iterations proceed the output gets closer and closer to a specific value.
convex_function,A real value function is called convex if the line segment between any two points on the graph of the function lies above or on the graph.
correlation,Correlation is the ratio of covariance of two variables to a product of variance (of the variables). It takes a value between +1 and -1. An extreme value on both the side means they are strongly correlated with each other. A value of zero indicates a NIL correlation but not a non-dependence. You’ll understand this clearly in one of the following answers.
cosine_similarity,Cosine Similarity is the cosine of the angle between 2 non-zero vectors. Two parallel vectors have a cosine similarity of 1 and two vectors at
cost_function,Cost function is used to define and measure the error of the model. The cost function is given by:
covariance,"Covariance is a measure of the joint variability of two random variables. It’s similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together. The formula for covariance is:"
cross_entropy,"In information theory, the cross entropy between two probability distributions and over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set, if a coding scheme is used that is optimized for an “unnatural” probability distribution , rather than the “true”. Cross entropy can be used to define the loss function in machine learning and optimization."
cross_validation,"Cross Validation is a technique which involves reserving a particular sample of a dataset which is not used to train the model. Later, the model is tested on this sample to evaluate the performance. There are various methods of performing cross validation such as:"
data_mining,Data mining is a study of extracting useful information from structured/unstructured data taken from various sources. This is done usually for
data_science,"Data science is a combination of data analysis, algorithmic development and technology in order to solve analytical problems. The main goal is a use of data to generate business value."
database,Database (abbreviated as DB) is an structured collection of data. The collected information is organised in a way such that it is easily accessible by the computer. Databases are built and managed by using database programming languages. The most common database language is SQL.
dataframe,"DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. DataFrame accepts many different kinds of input:"
dataset,"A dataset (or data set) is a collection of data. A dataset is organized into some type of data structure. In a database, for example, a dataset might contain a collection of business data (names, salaries, contact information, sales figures, and so forth). Several characteristics define a dataset’s structure and properties. These include the number and types of the attributes or variables, and various statistical measures applicable to them, such as standard deviation and kurtosis."
dashboard,"Dashboard is an information management tool which is used to visually track, analyze and display key performance indicators, metrics and key data points. Dashboards can be customised to fulfil the requirements of a project. It can be used to connect files, attachments, services and APIs which is displayed in the form of tables, line charts, bar charts and gauges. Popular tools for building dashboards include Excel and Tableau."
dbscan,DBSCAN is the acronym for
decision_boundary,"In a statistical-classification problem with two or more classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two or more sets, one for each class. How well the classifier works depends upon how closely the input patterns to be classified resemble the decision boundary. In the example sketched below, the correspondence is very close, and one can anticipate excellent performance."
descriptive_statistics,"Descriptive statistics is comprised of those values which explains the spread and central tendency of data. For example, mean is a way to represent central tendency of the data, whereas IQR is a way to represent spread of the data."
dependent_variable,"A dependent variable is what you measure and which is affected by independent / input variable(s). It is called dependent because it “depends” on the independent variable. For example, let’s say we want to predict the smoking habits of people. Then the person smokes “yes” or “no” is the dependent variable."
degree_of_freedom,It is the number of variables that have the choice of having more than one arbitrary value.
dimensionality_reduction,Dimensionality Reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. Dimension Reduction refers to the process of converting a set of data having vast dimensions into data with lesser dimensions ensuring that it conveys similar information concisely. Some of the benefits of dimensionality reduction:
dplyr,"Dplyr is a popular data manipulation package in R. It makes data manipulation, cleaning, summarizing very user friendly. Dplyr can work not only with the local datasets, but also with remote database tables, using exactly the same R code."
dummy_variable,Dummy Variable is another name for Boolean variable. An example of dummy variable is that it takes value 0 or 1. 0 means value is true (i.e. age < 25) and 1 means value is false (i.e. age >= 25)
early_stopping,"Early stopping is a technique for avoiding overfitting when training a machine learning model with iterative method. We set the early stopping in such a way that when the performance has stopped improving on the held-out validation set, the model training stops."
eda,EDA or exploratory data analysis is a phase used for data science pipeline in which the focus is to understand insights of the data through visualization or by statistical analysis.
etl,"ETL is the acronym for Extract, Transform and Load. An ETL system has the following properties:"
factor_analysis,Factor analysis is a technique that is used to reduce a large number of variables into fewer numbers of factors. Factor analysis aims to find independent latent variables. Factor analysis also assumes several assumptions:
false_negative,"Points which are actually true but are incorrectly predicted as false. For example, if the problem is to predict the loan status. (Y-loan approved, N-loan not approved). False negative in this case will be the samples for which loan was approved but the model predicted the status as not approved."
false_positive,"Points which are actually false but are incorrectly predicted as true. For example, if the problem is to predict the loan status. (Y-loan approved, N-loan not approved). False positive in this case will be the samples for which loan was not approved but the model predicted the status as approved."
feature_reduction,Feature reduction is the process of reducing the number of features to work on a computation intensive task without losing a lot of information.
flume,"Flume is a service designed for streaming logs into the Hadoop environment. It can collect and aggregate huge amounts of log data from a variety of sources. In order to collect high volume of data, multiple flume agents can be configured."
f_score,F-score evaluation metric combines both precision and recall as a measure of effectiveness of classification. It is calculated in terms of ratio of weighted importance on either recall or precision as determined by β coefficient.
gated_recurrent_unit_(gru),"The GRU is a variant of the LSTM (Long Short Term Memory) and was introduced by K. Cho. It retains the LSTM’s resistance to the vanishing gradient problem, but because of its simpler internal structure it is faster to train."
ggplot2,"GGplot2 is a data visualization package for the R programming language. It is a highly versatile and user-friendly tool for creating attractive plots. To know more about Ggplot2, visit"
go,"Go is an open source programming language that makes it easy to build simple, reliable, and efficient software. Go is a statically typed language in the tradition of C."
goodness_of_fit,The goodness of fit of a model describes how well it fits a set of observations. Measures of goodness of fit typically summarize the discrepancy between observed values and the values expected under the model.
gradient_descent,"Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. In machine learning algorithms, we use gradient descent to minimize the cost function. It find out the best set of parameters for our algorithm. Gradient Descent can be classified as follows:"
hadoop,Hadoop is an open source distributed processing framework used when we have to deal with enormous data. It allows us to use parallel processing capability to handle big data. Here are some significant benefits of Hadoop:
hidden_markov_model,"Hidden Markov Process is a Markov process in which the states are invisible or hidden, and the model developed to estimate these hidden states is known as the Hidden Markov Model (HMM). However, the output (data) dependent on the hidden states is visible. This output data generated by HMM gives some cue about the sequence of states."
hive,"Hive is a data warehouse software project to process structured data in Hadoop. It is built on top of Apache Hadoop for providing data summarization, query and analysis. Hive gives an SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop. Some of the key features of Hive are :"
holdout_sample,"While working on the dataset, a small part of the dataset is not used for training the model instead, it is used to check the performance of the model. This part of the dataset is called the holdout sample."
holt_winters_forecasting,Holt-Winters is one of the most popular forecasting techniques for time series. The model predicts the future values computing the combined effects of both trend and seasonality. The idea behind Holt’s Winter forecasting is to apply exponential smoothing to the seasonal components in addition to level and trend.
hyperparameter,A hyperparameter is a parameter whose value is set before training a machine learning or deep learning model. Different models require different hyperparameters and some require none. Hyperparameters should not be confused with the parameters of the model because the parameters are estimated or learned from the data.
hyperplane,"It is a subspace with one fewer dimensions than its surrounding area. If a space is 3-dimensional then its hyperplane is just a normal 2D plane. In 5 dimensional space, it’s a 4D plane, so on and so forth."
hypothesis,"Simply put, a hypothesis is a possible view or assertion of an analyst about the problem he or she is working upon. It may be true or may not be true.  Read more"
imputation,Imputation is a technique used for handling missing values in the data. This is done either by statistical metrics like mean/mode imputation or by machine learning techniques like kNN imputation
inferential_statistics,"In inferential statistics, we try to hypothesize about the population by only looking at a sample of it. For example, before releasing a drug in the market, internal tests are done to check if the drug is viable for release. But here we cannot check with the whole population for viability of the drug, so we do it on a sample which best represents the population."
iqr,IQR (or interquartile range) is a measure of variability based on dividing the rank-ordered data set into four equal parts. It can be derived by Quartile3 – Quartile1.
iteration,"Iteration refers to the number of times an algorithm’s parameters are updated while training a model on a dataset. For example, each iteration of training a neural network takes certain number of training data and updates the weights by using gradient descent or some other weight update rule."
julia,"Julia is a high-level, high-performance dynamic programming language for numerical computing. Some important features of Julia are:"
keras,"Keras is a simple, high-level neural network library, written in Python. It is capable of running on top of Tensorflow and Theano. This is done to make design and experiments with Neural Networks easier."
labeled_data,"A labeled dataset has a meaningful “label”, “class” or “tag” associated with each of its records or rows. For example, labels for a dataset of a set of images might be whether an image contains a cat or a dog."
line_chart,"Line charts are used to display information as series of points connected by straight line segment. These charts are used to communicate information visually, such as to show an increase or decrease in the trend in data over intervals of time."
log_loss,"Log Loss or Logistic loss is one of the evaluation metrics used to find how good the model is. Lower the log loss, better is the model. Log loss is the logarithm of the product of all probabilities."
logistic_regression,"In simple words, it predicts the probability of occurrence of an event by fitting data to a"
long_short_term_memory_(lstm),"Long short-term memory (LSTM) units (or blocks) are a building unit for layers of a recurrent neural network (RNN). A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell is responsible for “remembering” values over arbitrary time intervals, hence the word “memory” in LSTM. Each of the three gates can be thought of as a “conventional” artificial neuron, as in a multi-layer neural network, that is, they compute an activation (using an activation function) of a weighted sum. Applications of LSTM include:"
mahout,"Mahout is an open source project from Apache that is used for creating scalable machine learning algorithms. It implements popular machine learning techniques such as recommendation, classification, clustering."
mapreduce,"Hadoop MapReduce is a software framework for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner."
market_basket_analysis,Market Basket Analysis (also called as MBA) is a widely used technique among the Marketers to identify the best possible combinatory of the products or services which are frequently bought by the customers. This is also called product association analysis.Association analysis mostly done based on an algorithm named “Apriori Algorithm”. The Outcome of this analysis is called association rules. Marketers use these rules to strategize their recommendations.
market_mix_modeling,Market Mix Modeling is an analytical approach that uses historical information like point of sales to quantify the impact of some of the components on sales.
maximum_likelihood_estimation,It is a method for finding the values of parameters which make the likelihood maximum. The resulting values are called maximum likelihood estimates (MLE).
mean,"For a dataset, mean is said to be the average value of all the numbers. It can sometimes be used as a representation of the whole data."
median,"Median of a set of numbers is usually the middle value. When the total numbers in the set are even, the median will be the average of the two middle values. Median is used to measure the central tendency."
mis,"A management information system (MIS) is a computer system consisting of hardware and software that serves as the backbone of an organization’s operations. An MIS gathers data from multiple online systems, analyzes the information, and reports data to aid in management decision-making."
ml_as_a_service_(mlaas),"Machine learning as a service (MLaaS) is an array of services that provide machine learning tools as part of cloud computing services. This can include tools for data visualization, facial recognition, natural language processing, image recognition, predictive analytics, and deep learning. Some of the top ML-as-a-service providers are:"
mode,"Mode is the most frequent value occuring in the population. It is a metric to measure the central tendency, i.e. a way of expressing, in a (usually) single number, important information about a random variable or a population."
model_selection,Model selection is the task of selecting a statistical model from a set of known models. Various methods that can be used for choosing the model are:
monte_carlo_simluation,The idea behind Monte Carlo Simulation is to use random samples of parameters or inputs to explore the behavior of a complex process. Monte Carlo simulations sample from a probability distribution for each variable to produce hundreds or thousands of possible outcomes. The results are analyzed to get probabilities of different outcomes occurring.
multi_class_classification,Problems which have more than one class in the target variable are called multi-class Classification problems.
multivariate_analysis,Multivariate analysis is a process of comparing and analyzing the dependency of multiple variables over each other.
multivariate_regression,"Multivariate, as the word suggests, refers to ‘multiple dependent variables’. A regression model designed to deal with multiple dependent variables is called a multivariate regression model."
naive_bayes,"It is a classification technique based on Bayes’ theorem with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a fruit may be considered to be an apple if it is red, round and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier would consider all of these properties to independently contribute to the probability that this fruit is an apple."
natural_language_processing,"In simple words, Natural Language Processing is a field which aims to make computer systems understand human speech. NLP is comprised of techniques to process, structure, categorize raw text and extract information."
nosql,"NoSQL means Not only SQL. A NoSQL database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. It can accommodate a wide variety of data models, including key-value, document, columnar and graph formats."
nominal_variable,Nominal variables are
normalization,Normalization is the process of rescaling your data so that they have the same scale. Normalization is used when the attributes in our data have varying scales.
numpy,NumPy is the fundamental package for scientific computing with Python. It contains among other things:
one_shot_learning,It is a machine learning approach where the model is trained on a single example. One-shot Learning is generally used for object classification. This is performed to design effective classifiers from a single training example.
oozie,Apache Oozie is the tool in which all sort of programs can be pipelined in a desired order to work in Hadoop’s distributed environment. Oozie also provides a mechanism to run the job at a given schedule.
ordinal_variable,Ordinal variables are those variables which have discrete values but has some order involved. Refer
outlier,Outlier is an observation that appears far away and diverges from an overall pattern in a sample.
overfitting,A model is said to overfit when it performs well on the train dataset but fails on the test set. This happens when the model is too sensitive and captures random patterns which are present only in the training dataset. There are two methods to overcome overfitting:
pandas,"Pandas is an open source, high-performance, easy-to-use data structure and data analysis library for the Python programming language. Some of the highlights of Pandas are:"
parameters,"Parameters are a set of measurable factors that define a system. For machine learning models, model parameters are internal variables whose values can be determined from the data."
pattern_recognition,Pattern recognition is a branch of machine learning that focuses on the recognition of patterns and regularities in data. Classification is an example of pattern recognition wherein each input value is assigned one of a given set of classes.
pie_chart,"A pie chart is a circular statistical graphic which is divided into slices to illustrate numerical proportion. The arc length of each slice, is proportional to the quantity it represents. Let us understand it with an example:"
pig,"Pig is a high level scripting language that is used with Apache Hadoop. Pig enables data workers to write complex data transformations without knowing Java. Pig is complete, so one can do all required data manipulations in Apache Hadoop with Pig. Through the User Defined Functions(UDF) facility in Pig, Pig can invoke code in many languages like JRuby, Jython and Java."
predictor_variable,Predictor variable is used to make a prediction for dependent variables.
principal_component_analysis_(pca),"Principal component analysis (PCA) is an approach to factor analysis that considers the total variance in the data, and transforms the original variables into a smaller set of linear combinations. PCA is sensitive to outliers; they should be removed."
p_value,"P-value is the value of probability of getting a result equal to or greater than the observed value, when the null hypothesis is true."
python,"Python is an open source programming language, widely used for various applications, such as general purpose programming, data science and machine learning. Usually preferred by beginners in these fields because of the following major advantages:"
pytorch,"PyTorch is an open source machine learning library for python, based on Torch. It is built to provide flexibility as a deep learning development platform. Here are a few reasons for which PyTorch is extensively used :"
recommendation_engine,"Generally people tend to buy products recommended to them by their friends or the people they trust. Nowadays in the digital age, any online shop you visit utilizes some sort of recommendation engine. Recommendation engines basically are data filtering tools that make use of algorithms and data to recommend the most relevant items to a particular user. If we can recommend items to a customer based on their needs and interests, it will create a positive effect on the user experience and they will visit more frequently. There are few types of recommendation engines:"
regression_spline,"Regression Splines is a non-linear approach that uses a combination of linear/polynomial functions to fit the data. In this technique, instead of building one model for the entire dataset, it is divided into multiple bins and a separate model is built on each bin."
regularization,"Regularization is a technique used to solve the overfitting problem in statistical models. In machine learning, regularization penalizes the coefficients such that the model generalize better. We have different types of regression techniques which uses regularization such as Ridge regression and lasso regression."
residual,"Residual of a value is the difference between the observed value and the predicted value of the quantity of interest. Using the residual values, you can create residual plots which are useful for understanding the model."
response_variable,Response variable (or dependent variable) is that variable whose variation depends on other variables.
roc_auc,"Let’s first understand what is ROC (Receiver operating characteristic) curve. If we look at the confusion matrix, we observe that for a probabilistic model, we get different value for each metric."
root_mean_squared_error_(rmse),RMSE is a measure of the differences between values predicted by a model or an estimator and the values actually observed. It is the standard deviation of the residuals. Residuals are a measure of how far from the regression line data points are. The formula for RMSE is given by:
rotational_invariance,"In mathematics, a function defined on an inner product space is said to have rotational invariance if its value does not change when arbitrary rotations are applied to its argument."
scala,Scala is a general purpose language that combines concepts of object-oriented and functional programming languages. Here are some key features of Scala
smote,"It is a Synthetic Minority Over-Sampling Technique which is an approach to the construction of classifiers from imbalanced datasets is described. The idea behind this technique is that over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This is an over-sampling approach in which the minority class is over-sampled by creating “synthetic” examples rather than by over-sampling with replacement. To read further on SMOTE, refer"
spatial_temporal_reasoning,"Spatial-temporal reasoning is an area of artificial intelligence which draws from the fields of computer science, cognitive science, and cognitive psychology. Spatial-temporal reasoning is the ability to mentally move objects in space and time to solve multi-step problems. Three important things about Spatial-temporal reasoning are:"
standard_deviation,Standard deviation signifies how dispersed is the data. It is the square root of the variance of underlying data. Standard deviation is calculated for a population.
standardization,"Standardization (or Z-score normalization) is the process where the features are rescaled so that they’ll have the properties of a standard normal distribution with μ=0 and σ=1, where μ is the mean (average) and σ is the standard deviation from the mean. Standard scores (also called"
standard_error,"A standard error is the standard deviation of the sampling distribution of a statistic. The standard error is a statistical term that measures the accuracy of which a sample represents a population. In statistics, a sample mean deviates from the actual mean of a population this deviation is known as standard error."
statistics,"It is the study of the collection, analysis, interpretation, presentation, and organisation of data."
stochastic_gradient_descent,"Stochastic Gradient Descent is a type of gradient descent algorithm where we take a sample of data while computing the gradient. The update to the coefficients is performed for each training instance, rather than at the end of the batch of instances."
tensorflow,"TensorFlow, developed by the Google Brain team, is an open source software library. It is used for building machine learning models for range of tasks in data science, mainly used for machine learning applications such as building neural networks. TensorFlow can also be used for non- machine learning tasks that require numerical computation."
tokenization,Tokenization is the process of splitting a text string into units called tokens. The tokens may be words or a group of words. It is a crucial step in Natural Language Processing.
torch,"Torch is an open source machine learning library, based on the Lua programming language. It provides a wide range of algorithms for deep learning."
transfer_learning,Transfer learning refers to applying a pre-trained model on a new dataset. A pre-trained model is a model created by someone to solve a problem. This model can be applied to solve a similar problem with similar data.
true_negative,"These are the points which are actually false and we have predicted them false. For example, consider an example where we have to predict whether the loan will be approved or not. Y represents that loan will be approved, whereas N represents that loan will not be approved. So, here the True negative will be the number of classes which are actually N and we have predicted them N as well."
true_positive,"These are the points which are actually true and we have predicted them true. For example, consider an example where we have to predict whether the loan will be approved or not. Y represents that loan will be approved, whereas N represents that loan will not be approved. So, here the True positive will be the number of classes which are actually Y and we have predicted them Y as well."
type_i_error,"The decision to reject the null hypothesis could be incorrect, it is known as"
type_ii_error,"The decision to retain the null hypothesis could be incorrect, it is know as"
t_test,"T-test is used to compare two population by finding the difference of their population means. For more, refer"
underfitting,Underfitting occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the data. It refers to a model that can neither model on the training data nor generalize to new data. An underfit model is not a suitable model as it will have poor performance on the training data.
univariate_analysis,Univariate analysis is comparing and analyzing the dependency of a single predictor and a response variable
nan,"NaN stands for ‘not a number’. It is a numeric data type value representing an undefined or unrepresentable value. If the dataset has NaN values somewhere, it means that the data at that location is either missing or represented incorrectly."
z_test,"Z-test determines to what extent a data point is away from the mean of the data set, in standard deviation. For example:"
zookeeper,ZooKeeper is a software project of the Apache Software Foundation. It is an open source file application program interface (API) that allows distributed processes in large systems to synchronize with each other so that all clients making requests receive consistent data.
predictive_analytics,"Technology that learns from experience (data) to predict the outcome or behavior of individuals in order to drive better decisions. Predictive analytics is the use of machine learning for various commercial, industrial, and government applications. All applications of predictive analytics are applications of machine learning, and so the two terms are used somewhat interchangeably, depending on context. However, the reverse is not true: If you use machine learning to, for example, calculate the next best move playing checkers, or to solve certain engineering or signal processing problems, such as calculating the chances a high resolution photo has a traffic light within it, those uses of machine learning are rarely referred to as predictive analytics"
predictive_model,"A mechanism that predicts a behavior or outcome for an individual, such as click, buy, lie, or die. It takes characteristics of the individual as input (independent variables) and provides a predictive score as output, usually in the form of a probability. The higher the score, the more likely it is that the individual will exhibit the predicted behavior. Since it is generated by machine learning, we say that a predictive model is the thing that’s “learned”. Because of this, machine learning is also known as predictive modeling"
n_individual,"Within the definitions of predictive analytics and predictive model, individual is an intentionally broad term that can refer not only to individual people – such as customers, employees, voters, and healthcare patients – but also to other organizational elements, such as corporate clients, products, vehicles, buildings, manholes, transactions, social media posts, and much more. Whatever the domain, predictive analytics renders predictions over scalable numbers of individuals. That is, it operates at a lower level of granularity. This is what differentiates predictive analytics from forecasting"
training_data,"The data from which predictive modeling learns – that is, the data that machine learning software takes as input. It consists of a list (set) of [training] examples, aka training cases. For many business applications of machine learning, it is in the form of one row per example, each example corresponding to one individual"
labeled_data_training_data,"Training data  for which the correct answer is already known for each example, that is, for which the behavior or outcome being predicted is already labeled. This provides examples from which to learn, such as a list of customers, each one labeled as to whether or not they made a purchase. In some cases, labeling requires a manual effort, e.g., for determining whether an object such as a stop sign appears within a photograph or whether a certain healthcare condition is indicated within a medical image."
supervised_machine_learning_machine_learning,"that is guided by . The labels guide or “supervise” the learning process and also serve as the basis with which to evaluate a . Supervised machine learning is the most common form of machine learning and is the focus of this entire three-course series, so we will generally refer to it simply as machine learning."
unsupervised_machine_learning,"Methods that attempt to derive insights from unlabeled data. One common method is clustering, which groups examples together by some measure of similarity, trying to form groups that are as “cohesive” as possible. Since there are no correct answers – no labels – with which to assess the resulting groups, it’s generally a subjective choice as to how best to evaluate how good the results of learning are. This three-course series does not cover unsupervised learning (other than giving this definition and as a small part of a one-video case study on  within Course 3)."
forecasting,"Methods that make aggregate predictions on a macroscopic level that apply across . For example: How will the economy fare? Which presidential candidate will win more votes in Ohio? Forecasting estimates the overall total number of ice cream cones that will be purchased next month in Nebraska, while  tells you which individual Nebraskans are most likely to be seen with cone in hand."
data_science_/_big_data_/_analytics_/_data_mining,"Beyond “the clever use of data”, these subjective umbrella terms do not have agreed definitions. However, their various, competing definitions do generally include as a subtopic, as well as other forms of data analysis such as data visualization or, in some cases, just basic reporting. These terms do allude to a vital cultural movement led by thoughtful data wonks and other smart people doing creative things to make value of data. However, they don’t necessarily refer to any particular technology, method, or value proposition."
machine_learning_application,"A value proposition determined by two elements: 1) What’s predicted: the behavior or outcome to predict with a  for each individual, such as whether they’ll click, buy, lie, or die. And 2) What’s done about it: the operational decision to be driven for each individual by each corresponding prediction; that is, the action taken by the organization in response to or informed by each  output score, such as whether to contact, whether to approve for a credit card, or whether to investigate for fraud."
deployment,"The automation or support of operational decisions that is driven by the probabilistic scores output by a  – that is, the actual launch of the model. This requires the scores to be integrated into operations. For example, target a retention campaign to the top 5% of customers most likely to purchase if contacted."
decision_automation,The deployment of a  to drive a series of operational decisions automatically.
decision_support,"The deployment of a  to inform operational decisions made by a person. In their decision-making process, the person informally integrates or considers the model’s predictive scores in whatever ad hoc manner they see fit."
offline_deployment,"Scoring a batch job for which speed is not a great concern. For example, when selecting which customer to include for a direct marketing campaign, the computer can take more time, relatively speaking. Milliseconds are usually not a concern."
real_time_deployment,"Scoring as quickly as possible to inform an operational decision taking place in real time. For example, deciding which ad to show a customer at the moment a web page is loading means that the model must very quickly receive the customer’s as input and do its calculations so that the predictive score is then almost immediately available to the operational system."
the_prediction_effect,A little prediction goes a long way. Predicting better than guessing is very often more than sufficient to render mass scale operations more effective.
the_data_effect,"Data is always predictive. For all intents and purposes, virtually any given data set will reveal predictive insights. Leading UK consultant Tom Khabaza put it this way: “Projects never fail due to lack of patterns.” That is, other pitfalls may derail a machine learning project, but that generally won’t happen because of a lack of value in the data."
response_modeling,"For marketing, predictively modeling whether a customer will purchase if contacted in order to decide whether to include them for contact."
churn_modeling,"For marketing, predictively modeling whether a customer will leave (i.e., defect, cancel, attrite, or leave) in order to decide whether to extend a retention offer."
workforce_churn_modeling,"Predictively modeling whether an employee will leave (e.g., quit or be terminated) in order to to take measures to retain them or to plan accordingly."
credit_scoring,"For financial services, predictively modeling whether an individual debtor will default or become delinquent on a loan in order to decide whether to approve their application for credit, or to inform what APR and credit limit to offer or approve."
insurance_pricing_and_selection,Predictively modeling whether an individual will file high claims in order to decide whether to approve their application for insurance coverage (selection) or decide how to price their insurance policy (pricing).
fraud_detection,"Predictively modeling whether a transaction or application (e.g., for credit, benefits, or a tax refund) is fraudulent in order to decide whether to have a human auditor screen it."
product_recommendations,"Predictively modeling what next product the customer will buy or what media items such as video or music selections that customer would rate highly after consuming it, in order to decide which to recommend."
ad_targeting,Predictively modeling whether the customer will click on – or otherwise respond to – an online advertisement in order to decide which ad to display.
non_profit_fundraising,"Predictively modeling whether a prospect will donate if contacted in order to decide whether to include them for contact. This is the same value proposition as with  for marketing, except that “order fulfillment” is simpler: Rather than sending each responder a product, you only need to send a “thank you” note."
algorithmic_trading,Predictively modeling whether an asset’s value will go up or down in order to drive trading decisions.
predictive_policing,"Predictively modeling whether a suspect or convict will be arrested or convicted for a crime in order to inform investigation, bail, sentencing, or parole decisions. One typical modeling goal is to predict recidivism, that is, whether the individual will be re-arrested or re-convicted upon release from serving a jail sentence."
fault_detection,"For manufacturing, predictively modeling whether an item or product is defective – based on inputs from factory sensors – in order to decide whether to have it inspected by a human expert."
predictive_maintenance,Predictively modeling whether a vehicle or piece of equipment will fail or break down in order to decide whether to perform routine maintenance or otherwise inspect the item.
image_classification,Predictively modeling whether an image belongs to a certain category or depicts a certain item or object (aka ) within it in order to automatically flag the image accordingly. Applications of image classification include face recognition and medical image processing.
data_preparation,"The design and formation of the . This normally requires a specialized engineering and database programming effort, which must be heavily informed by business consideration, since the training data defines the functional intent of the  that will be generated from that data."
predictive_goal,"The thing that a model predicts, its target of prediction – that is, the outcome or behavior that the model will predict for each individual. For a given individual being predicted by the model, the score output by the model corresponds with the probability of this outcome or behavior. For example, this is a hypothetical predictive goal for : “Which current customers with a tenure of at least one year and who have purchased more than $500 to date will cancel within three months and not rejoin for another three months thereafter?”"
independent_variable,"A factor (i.e., a characteristic or attribute) known about an individual, such as a  like age or gender, or a  variable such as the number of prior purchases. A  takes independent variables as input."
binary_classifier.,"A  that predicts a “yes/no” , i.e., whether or not an  will exhibit the outcome or behavior being predicted. When predictively modeling on  with a  that has only two possible values, such as “yes” and “no” or “positive” and “negative”, the resulting model is a binary classifier. Binary classifiers suffice, at least as a first-pass approach, for most business applications of machine learning."
positive_and_negative_examples.,"In binary classification, the two possible outcomes or behaviors are usually signified as “positive” and “negative”, but it is somewhat arbitrary which is considered which. In most cases, the positive class is the less frequent class and is also the one that is more valuable to correctly identify, such as emails that are spam, medical images that signify the presence of a disease, or customers who will churn."
test_data,"Data that is held aside during the modeling process and used only to evaluate a model after the modeling is complete. The test data has the same variables as the , the same set of  and the same ."
demographic_data,"that characterize who an  is. These are inherent characteristics that are either immutable or tend not to change often, such as gender, age, ethnicity, aspects of the postal address, and billing details."
behavioral_data,"that summarize what an  has done or what has happened to that individual. This includes purchase behavior, online behavior, or any other observations of the individual’s actions."
derived_variable,"A manually-engineered inserted into the that is intended to provide value to the (typically, this means inserting a new column). A derived variable builds on other independent variables, extracting information through often simple mathematical operations."
feature_selection,"An automatic or semi-automatic pre-modeling phase that selects a favored subset of to be used for . After setting aside (filtering out) less valuable or redundant independent variables, the predictive modeling process has fewer independent variables to contend with and can “focus” only on a smaller number of valuable independent variables. This can result in a predictive model that exhibits higher performance."
uplift_modeling_predictive_modeling,"to predict the influence on an ‘s behavior or outcome that results from choosing one treatment over another. Instead of predicting the future, the behavior, whether there will be a positive outcome – as done by traditional predictive modeling – an uplift model predicts, “How much more likely is this treatment to result in the desired outcome than the alternative treatment?” For marketing, it predicts purchases  contact rather than contact."
induction,"The act of generalizing from examples, of leaping from a set of particulars to universals.  is a type of induction."
deduction,"The act of reasoning from the general to the particular, such as when applying known rules. For example, if all men are mortal and Socrates is a man, then deduction tells us Socrates is mortal. The application of a  to score an individual is an act of deduction, while the generation of the model in the first place is an act of . Induction ascertains new knowledge and deduction applies that knowledge. Induction almost always presents a greater challenge than deduction."
decision_boundaries,"The boundaries that represent how a  classifies individuals, when viewing the “space of individuals” as positioned on a two- or three-dimensional grid. This is a method to visually depict and help people gain an intuitive understanding of how a predictive model operates, the outward effects of its inner workings, what it mechanically accomplishes (without necessarily understanding how it works mathematically). When individuals are positioned within a higher dimensional space beyond two or three dimensions, that is, by considering more than three independent variables, it is not possible for humans to intuitively visualize. For that reason, this method is limited to only helping when a very small number of independent variables are in use."
automl,"software capabilities that automate some of the , , , selection of the  itself, and setting of the parameters for that choice of algorithm. While machine learning algorithms are themselves already automatic (by definition), autoML attempts to automate traditionally manual steps needed to set up and prepare for the use of those algorithms."
lift,"A multiplier – how many times more often the positive class occurs within a given segment defined by a , in comparison with the overall frequency of positive cases. We say that a  achieves a certain lift for a given segment. For example, “This model achieves a lift of three for the top 20%. If marketed to, the 20% of customers predicted as most likely to buy are three times more likely than average to purchase.”"
gains_curve,"A depiction of  performance with the horizontal axis signifying the proportion of examples considered, as ordered by model score, and the vertical axis signifying the proportion of all positive cases found therein. For example, for marketing, the x-axis represents how many of the ranked  are contacted, and the y-axis conveys the percent of all possible buyers found among those contacted. The gains curve corresponds with  since, at each position on the x-axis, the number of times higher the y-value is in comparison to the x-value equals the lift (equivalently, the number of times higher the curve is in comparison to the horizontally corresponding position on a straight diagonal line that extends from the bottom-left to the top-right equals the lift). Somewhat commonly, gains curves are incorrectly called “lift curves” – however, a lift curve is different. It has lift as its vertical axis, so it starts at the top-left and meanders down-right (lift curves are not covered in this course series)."
profit_curve,"A depiction of  performance with the same horizontal axis as a gains curve – signifying the proportion of examples considered – and with the vertical axis signifying profit. For example, for direct marketing, the x-axis is how many of the ranked individuals are to be contacted, and the y-axis is the profit that would be attained with that marketing campaign. To draw a profit curve, two business-side variables must be known: the cost per contact and the profit per positive case contacted."
misclassification_cost,"The penalty or price assigned to each  or . For example, in direct marketing, if it costs $2 to mail each customer a brochure, that is the false positive cost – if the model incorrectly designates a customer as a positive case, predicting that they will buy if contacted, the marketing campaign will spend the $2, but to no avail. And, if the average profit from each responsive customer is $100, that is the false negative cost – if the model incorrectly designates a customer as a negative case, the marketing campaign will neglect to contact that customer, and will thereby miss the opportunity to earn $100 from them. Misclassification costs form a basis for evaluating models – and, in some cases, for how modeling algorithms generate models, by designating the metric the algorithm is designed to optimize. In that way, costs can serve to define and determine what a machine learning project aims to optimize."
pairing_test,"An (often misleading) method to evaluate  that tests how often a model correctly distinguishes between a given pair of individuals, one positive and one negative. For example, if shown two images, one with a cat (meow) and one without a cat (no meow), how often will the model score the positive example more highly and thereby succeed in selecting between the two? This presumes the existence of such pairs, each already known to include one positive case and one negative case – however, the ability to manufacture such test pairs would require that the problem being approached with modeling has already been solved. A model’s performance on the pairing test is mathematically equal to its ."
auc,"A metric that indicates the extent of performance trade-offs available for a given . The higher the AUC, the better the trade-off options offered by the predictive model. The AUC is mathematically equal to the result you get running the . The AUC is outside the scope of this three-course series. It is a well-known but controversial metric."
roc,"A curve depicting the true positive rate vs. the false positive rate of a . The vertical axis, true positive rate, is the same as for a . Fluency with the ROC is outside the scope of this three-course series. It often visually appears somewhat similar to a gains curve in its shape, but the horizontal axis is the number of negative cases you’ve seen so far, rather than the total number of cases – positive or negative – as in a ."
kpi,"A measure of operational business performance that is key to a business’s strategy. Examples include revenue, sales, return on investment (ROI), marketing response rate, customer attrition rate, market penetration, and average wallet share."
strategic_objective,A  target used as a basis for reporting on the business improvements achieved by . Achieving a strategic objective by incorporating a  is a key selling point for that model. A strategic objective must define a KPI target that:
is_possible_to_estimate_a_priori,"Is measurable, in order to track ML success"
p_hacking,"Systematically trying out enough  – or, more generally, testing enough hypotheses – that you increase the risk of stumbling upon a false correlation that, when considered in isolation, appears to hold true, since it passes a test for statistical significance (i.e., shows a low p-value), albeit only by random chance. This leads to drawing a false conclusion, unless the number of variables tried out is taken into account when assessing the integrity of any given discovery/insight. The ultimate example of “torturing data until it confesses”, to p-hack is to try out too many variables/hypotheses, resulting in a high risk of being fooled by randomness. P-hacking is is a variation of , but rather than with complex models, it happens with very simple, one-variable models."
the_accuracy_fallacy,"When researchers report the high “” of a , but then later reveal – oftentimes buried within the details of a technical paper – that they were actually misusing the word “accuracy” to mean another measure of performance related to accuracy but in actuality not nearly as impressive, such as the  or the classification accuracy if half the cases were . This is a prevalent way in which machine learning performance is publicly misconstrued and greatly exaggerated, misleading people at large to falsely believe, for example, that machine learning can reliably predict whether you’re gay, whether you’ll develop psychosis, whether you’ll have a heart attack, whether you’re a criminal, and whether your unpublished book will be a bestseller."
presuming_that_correlation_implies_causation,"When operating on found data that has no control group, the unwarranted presumption of a causative relationship based only on an ascertained correlation. For example, if we observe that people who eat chocolate are thinner, we cannot jump to the presumption that eating chocolate actually keeps you thinner. It may be that people who are thin eat more chocolate because they weren’t concerned with losing weight in the first place, or any of a number of other plausible explanations. Instead, we must adhere to the well known adage, “Correlation does not imply causation.”"
optimizing_for_response_rate,"In marketing applications, conflating campaign response rate with campaign effectiveness. If many individuals who are targeted for contact do subsequently make a purchase, how do you know they wouldn’t have done so anyway, without spending the money to contact them? It may be that you’re targeting those likely to buy in any case – the “sure things” – more than those likely to be influenced by your marketing. The pitfall here is not only in how one evaluates the performance of targeted marketing, it is in whether one models the right thing in the first place. In many cases, a marketing campaign receives a lot more credit than it deserves. The remedy is to employ , which predicts a marketing treatment’s  on outcome rather than only predicting the outcome."
data_leak,"When an  gives away the . This is usually done inadvertently, but, informally, is referred to as “cheating”, since it means the model predicts based in part on the very thing it is predicting. This overblows the reported performance as evaluated on the, since that performance cannot be matched when going to , since the future will not be encoded within any independent variable (it cannot be, since it is not yet known). For example, if you’re doing , but an independent variable includes whether the customer received a marketing campaign contact that had only later been applied to customers who hadn’t cancelled their subscription, then the model will very quickly figure out that this is a helpful way to predict churn."
protected_class,"A  group designated to be protected from discrimination or bias.  that impart membership in a protected class include race, religion, national origin, gender, gender identity, sexual orientation, pregnancy, and disability status."
discriminatory_model,"A  that incorporates one or more independent variables that impart membership in a . By taking such a variable as an input, the model’s outputs – and the decisions driven by the model – are based at least in part on membership in a protected class. Exceptions apply when decisions are intended to benefit a protected class, such as for affirmative action, when determining whether one qualifies for a grant given to members of a protected class, or when determining medical treatment by gender."
machine_bias,"When a  exhibits disparate rates between protected classes, such as a crime risk model that exhibits a higher false positive rate for black defendants than for white defendants. While the word bias is a relatively broad and subjective criterion in its general usage, this definition applies consistently across this three-course series."
ground_truth.,"Objective reality that may or may not be captured by data, especially in relation to the . For example, if training data used to develop a crime risk model includes arrests or convictions for its dependent variable, there is a lack of ground truth, since arrests or convictions are only a proxy to whether an individual committed a crime (not all crimes are known and prosecuted)."
explainable_machine_learning,Technical methods to help humans understand how a  works.
model_transparency,"The standard that  be accessible, inspectable, and understandable. In some cases, this is only a matter of authorizing access to models for an auditing process that may include both model inspection and interrogation (experimentation). In other cases, it also requires the use of , since, once accessed, a more complex model may require special measures in order to decipher it."
the_right_to_explanation,"The standard that  be met when a consequential decision is driven or informed by a . For example, a defendant would be told which  contributed to their crime risk score – for which aspects of his or her background, circumstances, or past behavior the defendant was penalized. This provides the defendant the opportunity to respond accordingly, providing context, explanations, or perspective on these factors."
predatory_micro_targeting,"When ‘s increase in efficiency of activities designed to maximize profit leads to the disenfranchisement of consumers. Examples include when financial institutions are empowered to hold individuals in financial debt or when highly targeted advertisements are made increasingly adept at exploiting vulnerable consumers. In general, improving the micro-targeting of marketing and the predictive pricing of insurance and credit can magnify the cycle of poverty for consumers."
the_coded_gaze,"If a group of people such as a  is underrepresented in the training data used to form a , the model will, in general, not work as well for members of that group. This results in exclusionary experiences and discriminatory practices. This phenomenon can occur for both facial image processing and speech recognition."
